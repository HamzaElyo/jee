"""
Script d'entra√Ænement du mod√®le Random Forest pour la pr√©diction de risque √©tudiant
Bas√© sur le dataset OULAD (Open University Learning Analytics Dataset)
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
import joblib
import os

# Simuler des donn√©es OULAD pour l'entra√Ænement
# Dans un vrai cas, charger depuis MongoDB ou CSV
def generate_training_data(n_samples=5000):
    """
    G√©n√®re des donn√©es d'entra√Ænement simul√©es bas√©es sur OULAD
    Features: totalClicks, activeDays, numAssessments
    Target: riskLevel (Critical, High, Medium, Low)
    """
    np.random.seed(42)
    
    # G√©n√©rer des features
    data = {
        'totalClicks': np.random.exponential(scale=500, size=n_samples).astype(int),
        'activeDays': np.random.exponential(scale=30, size=n_samples).astype(int),
        'numAssessments': np.random.randint(0, 20, size=n_samples),
        'avgScore': np.random.normal(loc=60, scale=20, size=n_samples).clip(0, 100),
    }
    
    df = pd.DataFrame(data)
    
    # Calculer un score de risque bas√© sur les features
    risk_score = (
        (df['totalClicks'] < 200).astype(int) * 30 +
        (df['activeDays'] < 10).astype(int) * 25 +
        (df['numAssessments'] < 5).astype(int) * 25 +
        (df['avgScore'] < 50).astype(int) * 20
    )
    
    # Ajouter du bruit
    risk_score += np.random.randint(-10, 10, size=n_samples)
    risk_score = risk_score.clip(0, 100)
    
    # Convertir en cat√©gories
    def get_risk_level(score):
        if score >= 70: return 'Critical'
        elif score >= 50: return 'High'
        elif score >= 30: return 'Medium'
        else: return 'Low'
    
    df['riskLevel'] = risk_score.apply(get_risk_level)
    
    return df

def train_model():
    """Entra√Æne et sauvegarde le mod√®le Random Forest"""
    print("üìä G√©n√©ration des donn√©es d'entra√Ænement...")
    df = generate_training_data(5000)
    
    # Pr√©parer les features et le target
    X = df[['totalClicks', 'activeDays', 'numAssessments', 'avgScore']]
    y = df['riskLevel']
    
    # Encoder les labels
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    
    # Split train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    
    print("üå≤ Entra√Ænement du mod√®le Random Forest...")
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train, y_train)
    
    # √âvaluation
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"\n‚úÖ Accuracy: {accuracy:.2%}")
    print("\nüìà Classification Report:")
    print(classification_report(y_test, y_pred, target_names=le.classes_))
    
    # Sauvegarder le mod√®le et l'encoder
    print("\nüíæ Sauvegarde du mod√®le...")
    joblib.dump(model, 'model.pkl')
    joblib.dump(le, 'label_encoder.pkl')
    
    # Sauvegarder les importances des features
    feature_importance = dict(zip(X.columns, model.feature_importances_))
    print("\nüîç Importance des features:")
    for feat, imp in sorted(feature_importance.items(), key=lambda x: -x[1]):
        print(f"  {feat}: {imp:.3f}")
    
    print("\n‚úÖ Mod√®le sauvegard√©: model.pkl")
    print("‚úÖ Encoder sauvegard√©: label_encoder.pkl")

if __name__ == "__main__":
    train_model()
