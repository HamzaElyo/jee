"""
Script d'entraÃ®nement du modÃ¨le Random Forest pour la prÃ©diction de risque Ã©tudiant
Utilise les vraies donnÃ©es OULAD depuis les fichiers CSV
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
import joblib
import os

# Chemin vers les donnÃ©es OULAD
DATA_PATH = "../data/oulad"

def load_oulad_data():
    """
    Charge les donnÃ©es OULAD depuis les fichiers CSV
    """
    print("ğŸ“‚ Chargement des donnÃ©es OULAD depuis CSV...")
    
    # 1. Charger studentVle (activitÃ© des Ã©tudiants sur le VLE)
    print("   ğŸ“Š Chargement studentVle.csv (peut prendre un moment)...")
    vle_path = os.path.join(DATA_PATH, "studentVle.csv")
    student_vle = pd.read_csv(vle_path)
    
    # AgrÃ©ger par Ã©tudiant: totalClicks et activeDays
    print("   ğŸ”„ AgrÃ©gation des clicks par Ã©tudiant...")
    vle_agg = student_vle.groupby('id_student').agg({
        'sum_click': 'sum',
        'date': 'nunique'  # Nombre de jours uniques d'activitÃ©
    }).reset_index()
    vle_agg.columns = ['id_student', 'totalClicks', 'activeDays']
    
    print(f"   âœ… {len(vle_agg)} Ã©tudiants avec activitÃ© VLE")
    
    # 2. Charger studentInfo (infos Ã©tudiants avec finalResult)
    print("   ğŸ‘¥ Chargement studentInfo.csv...")
    info_path = os.path.join(DATA_PATH, "studentInfo.csv")
    student_info = pd.read_csv(info_path)
    
    # Garder les colonnes utiles
    student_info = student_info[['id_student', 'code_module', 'code_presentation', 
                                   'final_result', 'region', 'studied_credits']]
    
    print(f"   âœ… {len(student_info)} enregistrements Ã©tudiants")
    
    # 3. Charger studentAssessment (Ã©valuations)
    print("   ğŸ“ Chargement studentAssessment.csv...")
    assessment_path = os.path.join(DATA_PATH, "studentAssessment.csv")
    student_assessment = pd.read_csv(assessment_path)
    
    # AgrÃ©ger par Ã©tudiant: nombre d'Ã©valuations et score moyen
    assessment_agg = student_assessment.groupby('id_student').agg({
        'id_assessment': 'count',
        'score': 'mean'
    }).reset_index()
    assessment_agg.columns = ['id_student', 'numAssessments', 'avgScore']
    assessment_agg['avgScore'] = assessment_agg['avgScore'].fillna(50)
    
    print(f"   âœ… {len(assessment_agg)} Ã©tudiants avec Ã©valuations")
    
    # 4. Joindre toutes les donnÃ©es
    print("   ğŸ”— Fusion des donnÃ©es...")
    
    # Joindre VLE avec Info
    df = vle_agg.merge(student_info, on='id_student', how='inner')
    
    # Joindre avec Assessments
    df = df.merge(assessment_agg, on='id_student', how='left')
    
    # Remplir les valeurs manquantes
    df['numAssessments'] = df['numAssessments'].fillna(0).astype(int)
    df['avgScore'] = df['avgScore'].fillna(50.0)
    
    # 5. CrÃ©er les labels de risque basÃ©s sur final_result
    print("   ğŸ·ï¸ CrÃ©ation des labels de risque...")
    
    def get_risk_from_result(result):
        if result == "Withdrawn":
            return "Critical"
        elif result == "Fail":
            return "High"
        elif result == "Pass":
            return "Medium"
        elif result == "Distinction":
            return "Low"
        else:
            return "Medium"
    
    df["riskLevel"] = df["final_result"].apply(get_risk_from_result)
    
    # Supprimer les doublons (garder le premier enregistrement par Ã©tudiant)
    df = df.drop_duplicates(subset=['id_student'], keep='first')
    
    print(f"\nâœ… Dataset final: {len(df)} Ã©tudiants")
    print(f"   Distribution des risques:")
    print(df["riskLevel"].value_counts())
    
    return df


def train_model():
    """EntraÃ®ne et sauvegarde le modÃ¨le Random Forest avec donnÃ©es OULAD"""
    
    # Charger les vraies donnÃ©es
    df = load_oulad_data()
    
    if len(df) == 0:
        print("âŒ Aucune donnÃ©e trouvÃ©e!")
        return
    
    # PrÃ©parer les features et le target
    features = ['totalClicks', 'activeDays', 'numAssessments', 'avgScore']
    X = df[features]
    y = df['riskLevel']
    
    # Encoder les labels
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    
    print(f"\nğŸ“Š Features: {features}")
    print(f"ğŸ“Š Classes: {list(le.classes_)}")
    
    # Split train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    
    print(f"\nğŸ”€ Split: {len(X_train)} train, {len(X_test)} test")
    
    print("\nğŸŒ² EntraÃ®nement du modÃ¨le Random Forest...")
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train, y_train)
    
    # Ã‰valuation
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"\nâœ… Accuracy: {accuracy:.2%}")
    print("\nğŸ“ˆ Classification Report:")
    print(classification_report(y_test, y_pred, target_names=le.classes_))
    
    # Sauvegarder le modÃ¨le et l'encoder
    print("\nğŸ’¾ Sauvegarde du modÃ¨le...")
    joblib.dump(model, 'model.pkl')
    joblib.dump(le, 'label_encoder.pkl')
    
    # Afficher les importances des features
    feature_importance = dict(zip(features, model.feature_importances_))
    print("\nğŸ” Importance des features:")
    for feat, imp in sorted(feature_importance.items(), key=lambda x: -x[1]):
        print(f"   {feat}: {imp:.3f}")
    
    print("\nâœ… ModÃ¨le sauvegardÃ©: model.pkl")
    print("âœ… Encoder sauvegardÃ©: label_encoder.pkl")
    
    return model, le


if __name__ == "__main__":
    train_model()
