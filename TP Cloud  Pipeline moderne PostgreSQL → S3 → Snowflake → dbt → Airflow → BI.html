<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TP Cloud : Pipeline moderne PostgreSQL → S3 → Snowflake → dbt → Airflow → BI</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min. js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 10px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.2);
            overflow: hidden;
        }
        
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }
        
        header p {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 40px;
        }
        
        h2 {
            color: #667eea;
            font-size: 2em;
            margin: 40px 0 20px 0;
            padding-bottom: 10px;
            border-bottom: 3px solid #667eea;
        }
        
        h3 {
            color: #764ba2;
            font-size: 1.5em;
            margin: 30px 0 15px 0;
        }
        
        h4 {
            color: #555;
            font-size: 1.2em;
            margin: 20px 0 10px 0;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul, ol {
            margin-left: 30px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 10px;
        }
        
        pre {
            background: #f4f4f4;
            border-left: 4px solid #667eea;
            padding: 15px;
            overflow-x: auto;
            margin: 20px 0;
            border-radius: 5px;
            font-size: 0.9em;
        }
        
        code {
            background: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            color: #d63384;
        }
        
        pre code {
            background: transparent;
            padding: 0;
            color: #333;
        }
        
        .info-box {
            background: #e7f3ff;
            border-left: 5px solid #2196F3;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }
        
        .warning-box {
            background: #fff3cd;
            border-left: 5px solid #ffc107;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }
        
        .success-box {
            background: #d4edda;
            border-left: 5px solid #28a745;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .step-box {
            background: #f8f9fa;
            border: 2px solid #667eea;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }

        .step-number {
            display: inline-block;
            background: #667eea;
            color: white;
            width: 35px;
            height: 35px;
            line-height: 35px;
            text-align: center;
            border-radius: 50%;
            font-weight: bold;
            margin-right: 10px;
        }

        .click-instruction {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 10px 15px;
            margin: 10px 0;
            border-radius: 3px;
        }

        .click-instruction strong {
            color: #856404;
        }

        . pipeline-context {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            margin: 30px 0;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        .pipeline-context h3 {
            color: white;
            margin-top: 0;
        }

        .pipeline-steps {
            display: flex;
            justify-content: space-between;
            margin-top: 15px;
            flex-wrap: wrap;
        }

        .pipeline-step {
            background: rgba(255,255,255,0.2);
            padding: 10px 15px;
            border-radius: 5px;
            margin: 5px;
            flex: 1;
            min-width: 120px;
            text-align: center;
        }

        .pipeline-step. active {
            background: #28a745;
            font-weight: bold;
        }

        .file-path {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 10px 15px;
            border-radius: 5px;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
            border-left: 4px solid #4CAF50;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        
        table th, table td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        
        table th {
            background: #667eea;
            color: white;
        }
        
        table tr:nth-child(even) {
            background: #f9f9f9;
        }
        
        .mermaid {
            background: white;
            padding: 20px;
            border-radius: 5px;
            margin: 20px 0;
            text-align: center;
        }
        
        .diagram-caption {
            text-align: center;
            font-style: italic;
            color: #666;
            margin-top: 10px;
            font-size: 0.95em;
        }
        
        footer {
            background: #333;
            color: white;
            text-align: center;
            padding: 20px;
            font-size: 0.9em;
        }
        
        .toc {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 5px;
            margin: 20px 0;
        }
        
        .toc h3 {
            color: #667eea;
            margin-top: 0;
        }
        
        .toc ul {
            list-style: none;
            margin-left: 0;
        }
        
        .toc li {
            margin: 8px 0;
        }
        
        .toc a {
            color: #667eea;
            text-decoration: none;
            transition: color 0.3s;
        }
        
        .toc a:hover {
            color: #764ba2;
            text-decoration: underline;
        }
        
        .badge {
            display: inline-block;
            padding: 5px 10px;
            border-radius: 12px;
            font-size: 0.85em;
            font-weight: bold;
            margin-right: 5px;
        }
        
        .badge-tech {
            background: #667eea;
            color: white;
        }
        
        .badge-niveau {
            background: #28a745;
            color: white;
        }
        
        .badge-duree {
            background: #ffc107;
            color: #333;
        }

        .command-line {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 15px;
            border-radius: 5px;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
        }

        .command-line code {
            background: transparent;
            color: #f8f8f2;
        }
        
        @media print {
            body {
                background: white;
                padding: 0;
            }
            
            .container {
                box-shadow: none;
            }
            
            header {
                background: #667eea;
                -webkit-print-color-adjust: exact;
                print-color-adjust: exact;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>TP Cloud : Pipeline moderne</h1>
            <p>PostgreSQL → S3 → Snowflake → dbt → Airflow → BI</p>
        
        </header>
        
        <div class="content">
            <!-- Table des matières -->
            <div class="toc">
                <h3>Table des matières</h3>
                <ul>
                    <li><a href="#section1">1. Introduction au Modern Data Stack</a></li>
                    <li><a href="#section2">2.  Scénario métier : Plateforme SaaS E-commerce</a></li>
                    <li><a href="#section3">3. Architecture globale du pipeline</a></li>
                    <li><a href="#section4">4. Installation et configuration de PostgreSQL</a></li>
                    <li><a href="#section5">5.  Génération de données de démonstration</a></li>
                    <li><a href="#section6">6. Configuration d'Amazon S3</a></li>
                    <li><a href="#section7">7. Configuration de Snowflake</a></li>
                    <li><a href="#section8">8. Installation et utilisation de dbt</a></li>
                    <li><a href="#section9">9.  Création des Data Marts</a></li>
                    <li><a href="#section10">10. Installation et configuration d'Apache Airflow</a></li>
                    <li><a href="#section11">11. Connexion Power BI et création de dashboards</a></li>
                    <li><a href="#section12">12. Travail à rendre et livrables</a></li>
                </ul>
            </div>
            
            <!-- Section 1 -->
            <h2 id="section1">1. Introduction au Modern Data Stack</h2>
            
            <h3>1.1 Contexte : L'évolution vers le Cloud</h3>
            <p>
                Depuis une dizaine d'années, les entreprises migrent leurs infrastructures de données vers le <strong>cloud</strong>. 
                Cette évolution s'explique par plusieurs facteurs :
            </p>
            <ul>
                <li><strong>Scalabilité élastique</strong> : Les ressources s'adaptent automatiquement aux besoins (pics de charge, croissance des données)</li>
                <li><strong>Coûts optimisés</strong> : Modèle pay-as-you-go, pas d'investissement matériel initial</li>
                <li><strong>Maintenance réduite</strong> : Fini les serveurs à gérer, patches, sauvegardes physiques</li>
                <li><strong>Innovation rapide</strong> : Accès aux dernières technologies (ML, IA, streaming)</li>
                <li><strong>Collaboration facilitée</strong> : Accès global, travail à distance, partage de données</li>
            </ul>
            
            <div class="info-box">
                <strong>Le Modern Data Stack</strong><br>
                Le terme "Modern Data Stack" désigne un ensemble d'outils cloud-native, modulaires et intégrés, 
                qui permettent de construire rapidement des pipelines de données robustes et scalables.
            </div>
            
            <h3>1.2 OLTP vs OLAP : Rappel des concepts</h3>
            <table>
                <thead>
                    <tr>
                        <th>Critère</th>
                        <th>OLTP (Online Transaction Processing)</th>
                        <th>OLAP (Online Analytical Processing)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Objectif</strong></td>
                        <td>Traiter les transactions métier quotidiennes</td>
                        <td>Analyser les données pour la prise de décision</td>
                    </tr>
                    <tr>
                        <td><strong>Type de requêtes</strong></td>
                        <td>Lectures/écritures fréquentes, simples, rapides</td>
                        <td>Requêtes complexes, agrégations, jointures massives</td>
                    </tr>
                    <tr>
                        <td><strong>Volume de données</strong></td>
                        <td>Données actuelles, historique limité</td>
                        <td>Historique complet, plusieurs années</td>
                    </tr>
                    <tr>
                        <td><strong>Exemples</strong></td>
                        <td>PostgreSQL, MySQL, SQL Server</td>
                        <td>Snowflake, BigQuery, Redshift</td>
                    </tr>
                </tbody>
            </table>
            
            <!-- Section 2 -->
            <h2 id="section2">2. Scénario métier : Plateforme SaaS E-commerce</h2>
            
            <h3>2.1 Description de l'entreprise</h3>
            <p>
                Vous travaillez pour <strong>"ShopStream"</strong>, une plateforme SaaS de e-commerce en forte croissance. 
                ShopStream permet à des marchands de créer leur boutique en ligne, de gérer leurs produits, 
                de traiter les commandes et les paiements, et d'analyser leurs performances.
            </p>
            
            <h3>2. 2 Objectifs analytiques</h3>
            <ol>
                <li><strong>Chiffre d'affaires</strong> : CA total, par pays, par catégorie de produit</li>
                <li><strong>Funnel de conversion</strong> : Taux de conversion visite → inscription → commande</li>
                <li><strong>Customer Lifetime Value (CLV)</strong> : Valeur totale générée par un client</li>
                <li><strong>Performance produits</strong> : Top produits, catégories les plus vendues</li>
            </ol>
            
            <!-- Section 3 -->
            <h2 id="section3">3. Architecture globale du pipeline</h2>
            
            <h3>3.1 Diagramme d'architecture</h3>
            <div class="mermaid">
                graph TB

                subgraph Sources ["SOURCES DE DONNEES"]
                    PG(("PostgreSQL<br/>OLTP"))
                    LOGS["Logs JSON<br/>Events"]
                    CRM["CRM<br/>Salesforce"]
                end
            
                subgraph Ingestion ["INGESTION"]
                    FT["Scripts Python<br/>Extraction"]
                end
            
                subgraph Lake ["DATA LAKE"]
                    S3(("Amazon S3<br/>Raw Zone"))
                end
            
                subgraph DWH ["DATA WAREHOUSE"]
                    SF_STAGE["Snowflake<br/>STAGING"]
                    SF_CORE["Snowflake<br/>CORE / MARTS"]
                end
            
                subgraph Transform ["TRANSFORMATION"]
                    DBT["dbt<br/>SQL Models"]
                end
            
                subgraph Orchestration ["ORCHESTRATION"]
                    AIRFLOW["Apache Airflow<br/>DAGs"]
                end
            
                subgraph BI ["BUSINESS INTELLIGENCE"]
                    POWERBI["Power BI<br/>Dashboards"]
                end
            
                PG -->|Extract| FT
                LOGS -->|Collect| FT
                CRM -->|Sync| FT
            
                FT -->|Load Raw| S3
            
                S3 -->|COPY INTO| SF_STAGE
            
                SF_STAGE -->|dbt run| DBT
                DBT -->|Create / Update| SF_CORE
            
                SF_CORE -->|Query| POWERBI
            
                AIRFLOW -.->|Orchestrate| FT
                AIRFLOW -.->|Orchestrate| S3
                AIRFLOW -.->|Orchestrate| SF_STAGE
                AIRFLOW -.->|Orchestrate| DBT
                AIRFLOW -.->|Orchestrate| POWERBI
            
            </div>
            <p class="diagram-caption">Figure 1 : Architecture du Modern Data Stack pour ShopStream</p>
            
            <!-- Section 4 -->
            <h2 id="section4">4. Installation et configuration de PostgreSQL</h2>

            <div class="pipeline-context">
                <h3>Contexte dans le pipeline</h3>
                <p>Nous commençons par la première étape du pipeline : la mise en place de la source de données OLTP.</p>
                <div class="pipeline-steps">
                    <div class="pipeline-step active">PostgreSQL (OLTP)</div>
                    <div class="pipeline-step">S3 (Data Lake)</div>
                    <div class="pipeline-step">Snowflake (DWH)</div>
                    <div class="pipeline-step">dbt (Transform)</div>
                    <div class="pipeline-step">Airflow (Orchestration)</div>
                    <div class="pipeline-step">Power BI (BI)</div>
                </div>
            </div>
            
            <h3>4.1 Installation de PostgreSQL sur votre machine</h3>
            
            <div class="step-box">
                <h4><span class="step-number">1</span>Téléchargement de PostgreSQL</h4>
                
                <p><strong>Pour Windows :</strong></p>
                <ol>
                    <li>Ouvrez votre navigateur web</li>
                    <li>Allez sur <a href="https://www.postgresql. org/download/windows/" target="_blank">https://www. postgresql.org/download/windows/</a></li>
                    <li>Cliquez sur <strong>"Download the installer"</strong></li>
                    <li>Sélectionnez la version <strong>PostgreSQL 15.x</strong> pour Windows x86-64</li>
                    <li>Téléchargez le fichier (environ 250 Mo)</li>
                </ol>
                
                <p><strong>Pour macOS :</strong></p>
                <ol>
                    <li>Ouvrez votre Terminal</li>
                    <li>Installez Homebrew si ce n'est pas déjà fait :
                        <div class="command-line">
                            <code>/bin/bash -c "$(curl -fsSL https://raw. githubusercontent.com/Homebrew/install/HEAD/install.sh)"</code>
                        </div>
                    </li>
                    <li>Installez PostgreSQL :
                        <div class="command-line">
                            <code>brew install postgresql@15</code>
                        </div>
                    </li>
                    <li>Démarrez PostgreSQL :
                        <div class="command-line">
                            <code>brew services start postgresql@15</code>
                        </div>
                    </li>
                </ol>
                
                <p><strong>Pour Linux (Ubuntu/Debian) :</strong></p>
                <div class="command-line">
                    <code>sudo apt update</code><br>
                    <code>sudo apt install postgresql postgresql-contrib</code><br>
                    <code>sudo systemctl start postgresql</code><br>
                    <code>sudo systemctl enable postgresql</code>
                </div>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">2</span>Installation de PostgreSQL (Windows - suite)</h4>
                
                <ol>
                    <li>Double-cliquez sur le fichier téléchargé <code>postgresql-15.x-windows-x64.exe</code></li>
                    <li>L'assistant d'installation s'ouvre</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Next"</strong></div></li>
                    <li>Sélectionnez le dossier d'installation (laisser par défaut : <code>C:\Program Files\PostgreSQL\15</code>)</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Next"</strong></div></li>
                    <li>Sélectionnez les composants à installer (cochez TOUT) :
                        <ul>
                            <li>PostgreSQL Server</li>
                            <li>pgAdmin 4</li>
                            <li>Stack Builder</li>
                            <li>Command Line Tools</li>
                        </ul>
                    </li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Next"</strong></div></li>
                    <li>Sélectionnez le dossier de données (laisser par défaut)</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Next"</strong></div></li>
                    <li><strong>IMPORTANT</strong> : Définissez un mot de passe pour l'utilisateur <code>postgres</code>
                        <ul>
                            <li>Exemple : <code>postgres123</code></li>
                            <li><strong>Notez ce mot de passe quelque part, vous en aurez besoin</strong></li>
                        </ul>
                    </li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Next"</strong></div></li>
                    <li>Port par défaut : <code>5432</code> (laisser tel quel)</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Next"</strong></div></li>
                    <li>Locale : <code>French, France</code></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Next"</strong></div></li>
                    <li>Vérifiez le résumé de l'installation</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Next"</strong></div></li>
                    <li>L'installation démarre (patientez 2-3 minutes)</li>
                    <li>Une fois terminé, <strong>décochez</strong> "Launch Stack Builder at exit"</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Finish"</strong></div></li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">3</span>Vérification de l'installation</h4>
                
                <p><strong>Sous Windows :</strong></p>
                <ol>
                    <li>Ouvrez le menu Démarrer</li>
                    <li>Tapez <code>pgAdmin</code></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"pgAdmin 4"</strong></div></li>
                    <li>pgAdmin s'ouvre dans votre navigateur (peut prendre 10-20 secondes)</li>
                    <li>Dans le panneau gauche, vous voyez <strong>"Servers"</strong></li>
                    <li><div class="click-instruction">Cliquez sur le triangle à côté de <strong>"Servers"</strong></div></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"PostgreSQL 15"</strong></div></li>
                    <li>Une fenêtre demande le mot de passe</li>
                    <li>Entrez le mot de passe que vous avez défini (exemple : <code>postgres123</code>)</li>
                    <li>Cochez <strong>"Save password"</strong></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"OK"</strong></div></li>
                    <li>Si vous voyez apparaître <strong>"Databases"</strong>, l'installation est réussie</li>
                </ol>
                
                <p><strong>Test en ligne de commande :</strong></p>
                <ol>
                    <li>Ouvrez une invite de commande (CMD) ou Terminal</li>
                    <li>Tapez la commande suivante :
                        <div class="command-line">
                            <code>psql --version</code>
                        </div>
                    </li>
                    <li>Vous devez voir quelque chose comme : <code>psql (PostgreSQL) 15.4</code></li>
                    <li>PostgreSQL est bien installé</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">4</span>Création de la base de données ShopStream</h4>
                
                <p><strong>Méthode 1 : Via pgAdmin (interface graphique)</strong></p>
                <ol>
                    <li>Dans pgAdmin, développez l'arbre : <strong>Servers > PostgreSQL 15</strong></li>
                    <li><div class="click-instruction">Faites un clic droit sur <strong>"Databases"</strong></div></li>
                    <li><div class="click-instruction">Sélectionnez <strong>"Create" > "Database..."</strong></div></li>
                    <li>Une fenêtre s'ouvre</li>
                    <li>Dans le champ <strong>"Database"</strong>, tapez : <code>shopstream</code></li>
                    <li>Laissez les autres champs par défaut</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Save"</strong></div></li>
                    <li>La base <code>shopstream</code> apparaît dans la liste des bases de données</li>
                </ol>
                
                <p><strong>Méthode 2 : Via ligne de commande</strong></p>
                <div class="command-line">
                    <code>createdb -U postgres shopstream</code>
                </div>
                <p>Entrez le mot de passe postgres quand demandé.</p>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">5</span>Création des tables PostgreSQL</h4>
                
                <ol>
                    <li>Dans pgAdmin, développez : <strong>Servers > PostgreSQL 15 > Databases > shopstream</strong></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"shopstream"</strong> pour la sélectionner</div></li>
                    <li>En haut de l'interface, vous voyez une barre d'outils</li>
                    <li><div class="click-instruction">Cliquez sur l'icône <strong>"Query Tool"</strong> (icône avec un éclair)</div></li>
                    <li>Un éditeur SQL s'ouvre dans le panneau de droite</li>
                    <li>Copiez-collez le script SQL suivant :</li>
                </ol>

<pre><code class="language-sql">-- ============================================
-- Script de création du schéma OLTP PostgreSQL
-- Plateforme ShopStream
-- ============================================

-- 1. Table USERS
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    country VARCHAR(3),
    plan_type VARCHAR(20) DEFAULT 'freemium',
    created_at TIMESTAMP DEFAULT NOW(),
    last_login TIMESTAMP,
    is_active BOOLEAN DEFAULT TRUE
);

CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_country ON users(country);
CREATE INDEX idx_users_plan_type ON users(plan_type);

-- 2. Table PRODUCTS
CREATE TABLE products (
    id SERIAL PRIMARY KEY,
    merchant_id INT NOT NULL,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    category VARCHAR(100),
    price DECIMAL(10,2) NOT NULL,
    stock_quantity INT DEFAULT 0,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_products_merchant ON products(merchant_id);
CREATE INDEX idx_products_category ON products(category);

-- 3. Table ORDERS
CREATE TABLE orders (
    id SERIAL PRIMARY KEY,
    user_id INT NOT NULL REFERENCES users(id),
    created_at TIMESTAMP DEFAULT NOW(),
    total_amount DECIMAL(10,2) NOT NULL,
    status VARCHAR(20) DEFAULT 'pending',
    country VARCHAR(3),
    payment_method VARCHAR(50)
);

CREATE INDEX idx_orders_user_id ON orders(user_id);
CREATE INDEX idx_orders_created_at ON orders(created_at);
CREATE INDEX idx_orders_status ON orders(status);

-- 4. Table ORDER_ITEMS
CREATE TABLE order_items (
    id SERIAL PRIMARY KEY,
    order_id INT NOT NULL REFERENCES orders(id),
    product_id INT NOT NULL REFERENCES products(id),
    quantity INT NOT NULL,
    unit_price DECIMAL(10,2) NOT NULL,
    line_total DECIMAL(10,2) NOT NULL
);

CREATE INDEX idx_order_items_order_id ON order_items(order_id);
CREATE INDEX idx_order_items_product_id ON order_items(product_id);

-- 5.  Table EVENTS (logs applicatifs)
CREATE TABLE events (
    id BIGSERIAL PRIMARY KEY,
    user_id INT REFERENCES users(id),
    event_type VARCHAR(50) NOT NULL,
    event_ts TIMESTAMP DEFAULT NOW(),
    metadata JSONB
);

CREATE INDEX idx_events_user_id ON events(user_id);
CREATE INDEX idx_events_event_type ON events(event_type);
CREATE INDEX idx_events_event_ts ON events(event_ts);

-- 6. Table CRM_CONTACTS
CREATE TABLE crm_contacts (
    id SERIAL PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    source VARCHAR(100),
    campaign_id VARCHAR(100),
    created_at TIMESTAMP DEFAULT NOW(),
    converted BOOLEAN DEFAULT FALSE,
    converted_at TIMESTAMP
);

CREATE INDEX idx_crm_contacts_email ON crm_contacts(email);
CREATE INDEX idx_crm_contacts_source ON crm_contacts(source);
</code></pre>

                <ol start="7">
                    <li><div class="click-instruction">Cliquez sur le bouton <strong>"Execute/Play"</strong> en haut de l'éditeur</div></li>
                    <li>Le script s'exécute (durée : quelques secondes)</li>
                    <li>En bas, vous voyez le message : <code>Query returned successfully in XXX msec. </code></li>
                    <li>Les tables sont créées</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">6</span>Vérification de la création des tables</h4>
                
                <ol>
                    <li>Dans le panneau gauche de pgAdmin, développez :
                        <ul>
                            <li><strong>Servers > PostgreSQL 15 > Databases > shopstream > Schemas > public > Tables</strong></li>
                        </ul>
                    </li>
                    <li><div class="click-instruction">Cliquez sur le triangle à côté de <strong>"Tables"</strong></div></li>
                    <li>Vous devez voir les 6 tables :
                        <ul>
                            <li>users</li>
                            <li>products</li>
                            <li>orders</li>
                            <li>order_items</li>
                            <li>events</li>
                            <li>crm_contacts</li>
                        </ul>
                    </li>
                    <li>Votre base de données est prête</li>
                </ol>
            </div>

            <h3>4.2 Schéma entité-relation (ERD)</h3>
            <div class="mermaid">
erDiagram
    USERS ||--o{ ORDERS : places
    USERS {
        int id PK
        varchar email UK
        varchar country
        varchar plan_type
        timestamp created_at
    }
    
    PRODUCTS ||--o{ ORDER_ITEMS : contains
    PRODUCTS {
        int id PK
        varchar name
        varchar category
        decimal price
        int merchant_id
    }
    
    ORDERS ||--|{ ORDER_ITEMS : has
    ORDERS {
        int id PK
        int user_id FK
        timestamp created_at
        decimal total_amount
        varchar status
    }
    
    ORDER_ITEMS {
        int id PK
        int order_id FK
        int product_id FK
        int quantity
        decimal unit_price
    }
    
    USERS ||--o{ EVENTS : generates
    EVENTS {
        bigint id PK
        int user_id FK
        varchar event_type
        timestamp event_ts
    }
    
    CRM_CONTACTS {
        int id PK
        varchar email UK
        varchar source
        timestamp created_at
    }
            </div>
            <p class="diagram-caption">Figure 2 : Schéma entité-relation de la base PostgreSQL</p>
            
            <!-- Section 5 -->
            <h2 id="section5">5. Génération de données de démonstration</h2>

            <div class="pipeline-context">
                <h3>Contexte dans le pipeline</h3>
                <p>Nous sommes toujours sur la première étape : PostgreSQL.  Maintenant que la base est créée, nous allons la remplir avec des données réalistes.</p>
                <div class="pipeline-steps">
                    <div class="pipeline-step active">PostgreSQL (OLTP)</div>
                    <div class="pipeline-step">S3 (Data Lake)</div>
                    <div class="pipeline-step">Snowflake (DWH)</div>
                    <div class="pipeline-step">dbt (Transform)</div>
                    <div class="pipeline-step">Airflow (Orchestration)</div>
                    <div class="pipeline-step">Power BI (BI)</div>
                </div>
            </div>
            
            <h3>5.1 Installation de Python et des bibliothèques nécessaires</h3>
            
            <div class="step-box">
                <h4><span class="step-number">1</span>Installation de Python</h4>
                
                <p><strong>Vérification si Python est déjà installé :</strong></p>
                <ol>
                    <li>Ouvrez une invite de commande (CMD sous Windows, Terminal sous macOS/Linux)</li>
                    <li>Tapez :
                        <div class="command-line">
                            <code>python --version</code>
                        </div>
                    </li>
                    <li>Si vous voyez <code>Python 3.8</code> ou supérieur, passez à l'étape suivante</li>
                    <li>Sinon, continuez ci-dessous</li>
                </ol>
                
                <p><strong>Installation de Python (Windows) :</strong></p>
                <ol>
                    <li>Allez sur <a href="https://www.python. org/downloads/" target="_blank">https://www. python.org/downloads/</a></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Download Python 3.11. x"</strong></div></li>
                    <li>Double-cliquez sur le fichier téléchargé</li>
                    <li><strong>IMPORTANT</strong> : Cochez <strong>"Add Python to PATH"</strong></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Install Now"</strong></div></li>
                    <li>Patientez pendant l'installation (2-3 minutes)</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Close"</strong></div></li>
                    <li>Fermez et rouvrez votre invite de commande</li>
                    <li>Vérifiez : <code>python --version</code></li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">2</span>Installation des bibliothèques Python</h4>
                
                <ol>
                    <li>Ouvrez une invite de commande (CMD ou Terminal)</li>
                    <li>Installez les bibliothèques nécessaires :
                        <div class="command-line">
                            <code>pip install faker psycopg2-binary pandas</code>
                        </div>
                    </li>
                    <li>L'installation démarre (durée : 1-2 minutes)</li>
                    <li>Vous voyez défiler les messages d'installation</li>
                    <li>À la fin, vous devez voir : <code>Successfully installed faker-...  psycopg2-binary-... pandas-...</code></li>
                    <li>Les bibliothèques sont installées</li>
                </ol>
                
                <div class="info-box">
                    <strong>Rôle des bibliothèques :</strong>
                    <ul>
                        <li><strong>faker</strong> : Génère des données réalistes (noms, emails, adresses... )</li>
                        <li><strong>psycopg2</strong> : Connecteur Python pour PostgreSQL</li>
                        <li><strong>pandas</strong> : Manipulation de données (optionnel mais utile)</li>
                    </ul>
                </div>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">3</span>Création du dossier de projet</h4>
                
                <p><strong>Emplacement recommandé :</strong></p>
                <ul>
                    <li><strong>Windows :</strong> <code>C:\Users\VotreNom\ShopStreamTP</code></li>
                    <li><strong>macOS/Linux :</strong> <code>~/ShopStreamTP</code></li>
                </ul>
                
                <p><strong>Création du dossier :</strong></p>
                <p><em>Sous Windows :</em></p>
                <ol>
                    <li>Ouvrez l'Explorateur de fichiers</li>
                    <li>Naviguez vers <code>C:\Users\VotreNom\</code></li>
                    <li>Clic droit > Nouveau > Dossier</li>
                    <li>Nommez-le : <code>ShopStreamTP</code></li>
                </ol>
                
                <p><em>Sous macOS/Linux :</em></p>
                <div class="command-line">
                    <code>mkdir ~/ShopStreamTP</code><br>
                    <code>cd ~/ShopStreamTP</code>
                </div>

                <p><strong>Structure de dossiers à créer :</strong></p>
                <pre><code>ShopStreamTP/
├── scripts/          (à créer maintenant)
├── dbt/             (à créer plus tard)
├── airflow/         (à créer plus tard)
└── docs/            (à créer plus tard)</code></pre>

                <p><em>Création du sous-dossier scripts :</em></p>
                <p>Sous Windows : Créez un sous-dossier <code>scripts</code> dans <code>ShopStreamTP</code></p>
                <p>Sous macOS/Linux :</p>
                <div class="command-line">
                    <code>mkdir ~/ShopStreamTP/scripts</code>
                </div>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">4</span>Création du script de génération de données</h4>
                
                <div class="file-path">
                    <strong>Emplacement du fichier :</strong><br>
                    Windows : <code>C:\Users\VotreNom\ShopStreamTP\scripts\generate_data.py</code><br>
                    macOS/Linux : <code>~/ShopStreamTP/scripts/generate_data.py</code>
                </div>
                
                <ol>
                    <li>Ouvrez un éditeur de texte (Notepad++, VS Code, Sublime Text, ou Bloc-notes)</li>
                    <li>Copiez-collez le script Python ci-dessous</li>
                    <li>Sauvegardez le fichier sous le nom <code>generate_data. py</code> dans le dossier <code>ShopStreamTP/scripts/</code></li>
                </ol>

<pre><code class="language-python">"""
generate_data.py
Script de génération de données pour ShopStream
Emplacement : ShopStreamTP/scripts/generate_data.py
"""

import random
import json
from datetime import datetime, timedelta
from faker import Faker
import psycopg2
from psycopg2.extras import execute_batch

# Configuration de Faker (multi-langues pour réalisme)
fake = Faker(['fr_FR', 'en_US', 'de_DE', 'es_ES'])

# MODIFIEZ CES PARAMETRES SELON VOTRE CONFIGURATION
DB_CONFIG = {
    'host': 'localhost',
    'database': 'shopstream',
    'user': 'postgres',
    'password': 'postgres123'  # METTEZ VOTRE MOT DE PASSE ICI
}

# Constantes métier
COUNTRIES = ['FRA', 'USA', 'DEU', 'ESP', 'GBR', 'ITA', 'CAN', 'AUS']
PLAN_TYPES = ['freemium', 'premium', 'enterprise']
CATEGORIES = ['Electronics', 'Fashion', 'Home', 'Books', 'Sports', 'Beauty', 'Toys']
ORDER_STATUSES = ['pending', 'paid', 'shipped', 'delivered', 'cancelled']
EVENT_TYPES = ['page_view', 'add_to_cart', 'checkout_start', 'purchase', 'error']
PAYMENT_METHODS = ['stripe', 'paypal', 'credit_card']
SOURCES = ['organic', 'paid_ads', 'referral', 'email_campaign']

def get_connection():
    """Connexion à PostgreSQL"""
    print("Connexion à PostgreSQL...")
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        print("Connexion réussie")
        return conn
    except Exception as e:
        print(f"Erreur de connexion : {e}")
        print("Vérifiez vos paramètres dans DB_CONFIG (host, database, user, password)")
        exit(1)

def generate_users(conn, n=1000):
    """Génère n utilisateurs"""
    print(f"\nGénération de {n} utilisateurs...")
    cursor = conn.cursor()
    
    users = []
    for i in range(n):
        if i % 100 == 0:
            print(f"   ...  {i}/{n} utilisateurs générés")
        
        user = (
            fake.email(),
            fake.first_name(),
            fake.last_name(),
            random.choice(COUNTRIES),
            random.choice(PLAN_TYPES) if random.random() > 0.7 else 'freemium',
            fake.date_time_between(start_date='-2y', end_date='now'),
            fake.date_time_between(start_date='-30d', end_date='now') if random.random() > 0.3 else None,
            random. random() > 0.05
        )
        users.append(user)
    
    query = """
        INSERT INTO users (email, first_name, last_name, country, plan_type, created_at, last_login, is_active)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
    """
    
    try:
        execute_batch(cursor, query, users, page_size=100)
        conn.commit()
        print(f"{n} utilisateurs créés avec succès")
    except Exception as e:
        print(f"Erreur lors de la création des utilisateurs : {e}")
        conn.rollback()

def generate_products(conn, n=200):
    """Génère n produits"""
    print(f"\nGénération de {n} produits...")
    cursor = conn.cursor()
    
    products = []
    for i in range(n):
        if i % 50 == 0:
            print(f"   ... {i}/{n} produits générés")
        
        merchant_id = random.randint(1, 100)
        product = (
            merchant_id,
            fake.catch_phrase(),
            fake.text(max_nb_chars=150),
            random.choice(CATEGORIES),
            round(random.uniform(5. 0, 500.0), 2),
            random.randint(0, 1000),
            fake.date_time_between(start_date='-1y', end_date='now'),
            datetime.now()
        )
        products.append(product)
    
    query = """
        INSERT INTO products (merchant_id, name, description, category, price, stock_quantity, created_at, updated_at)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
    """
    
    try:
        execute_batch(cursor, query, products, page_size=100)
        conn.commit()
        print(f"{n} produits créés avec succès")
    except Exception as e:
        print(f"Erreur lors de la création des produits : {e}")
        conn.rollback()

def generate_orders(conn, n=5000):
    """Génère n commandes avec leurs lignes"""
    print(f"\nGénération de {n} commandes...")
    cursor = conn.cursor()
    
    # Récupération des IDs users et products existants
    cursor.execute("SELECT id, country FROM users WHERE is_active = TRUE")
    users = cursor.fetchall()
    
    cursor. execute("SELECT id, price FROM products")
    products = cursor.fetchall()
    
    if not users or not products:
        print("Pas d'utilisateurs ou de produits.  Générez-les d'abord.")
        return
    
    orders = []
    order_items = []
    
    for i in range(n):
        if i % 500 == 0:
            print(f"   ... {i}/{n} commandes générées")
        
        user = random.choice(users)
        user_id = user[0]
        country = user[1]
        
        created_at = fake.date_time_between(start_date='-6m', end_date='now')
        
        # Nombre d'articles par commande (1 à 5)
        nb_items = random.randint(1, 5)
        
        total_amount = 0
        order_products = random.sample(products, min(nb_items, len(products)))
        
        for product in order_products:
            product_id = product[0]
            unit_price = product[1]
            quantity = random.randint(1, 3)
            line_total = unit_price * quantity
            total_amount += line_total
            
            order_items.append((
                i + 1,
                product_id,
                quantity,
                unit_price,
                line_total
            ))
        
        status = random.choices(
            ORDER_STATUSES,
            weights=[5, 40, 25, 25, 5]
        )[0]
        
        orders.append((
            user_id,
            created_at,
            round(total_amount, 2),
            status,
            country,
            random.choice(PAYMENT_METHODS)
        ))
    
    # Insertion des commandes
    query_orders = """
        INSERT INTO orders (user_id, created_at, total_amount, status, country, payment_method)
        VALUES (%s, %s, %s, %s, %s, %s)
    """
    
    try:
        execute_batch(cursor, query_orders, orders, page_size=100)
        conn.commit()
        print(f"{n} commandes créées")
    except Exception as e:
        print(f"Erreur lors de la création des commandes : {e}")
        conn.rollback()
        return
    
    # Insertion des lignes de commande
    query_items = """
        INSERT INTO order_items (order_id, product_id, quantity, unit_price, line_total)
        VALUES (%s, %s, %s, %s, %s)
    """
    
    try:
        execute_batch(cursor, query_items, order_items, page_size=100)
        conn.commit()
        print(f"{len(order_items)} lignes de commande créées")
    except Exception as e:
        print(f"Erreur lors de la création des lignes : {e}")
        conn.rollback()

def generate_events(conn, n=10000):
    """Génère n événements"""
    print(f"\nGénération de {n} événements...")
    cursor = conn.cursor()
    
    cursor.execute("SELECT id FROM users LIMIT 500")
    user_ids = [row[0] for row in cursor.fetchall()]
    
    if not user_ids:
        print("Pas d'utilisateurs. Générez-les d'abord.")
        return
    
    events = []
    for i in range(n):
        if i % 1000 == 0:
            print(f"   ... {i}/{n} événements générés")
        
        user_id = random.choice(user_ids) if random.random() > 0.1 else None
        event_type = random.choice(EVENT_TYPES)
        event_ts = fake.date_time_between(start_date='-3m', end_date='now')
        
        if event_type == 'page_view':
            metadata = json.dumps({
                'page_url': fake.uri_path(),
                'device': random.choice(['mobile', 'desktop', 'tablet'])
            })
        elif event_type == 'add_to_cart':
            metadata = json.dumps({
                'product_id': random.randint(1, 200),
                'quantity': random.randint(1, 3)
            })
        else:
            metadata = json.dumps({})
        
        events.append((
            user_id,
            event_type,
            event_ts,
            metadata
        ))
    
    query = """
        INSERT INTO events (user_id, event_type, event_ts, metadata)
        VALUES (%s, %s, %s, %s)
    """
    
    try:
        execute_batch(cursor, query, events, page_size=100)
        conn.commit()
        print(f"{n} événements créés")
    except Exception as e:
        print(f"Erreur lors de la création des événements : {e}")
        conn.rollback()

def generate_crm_contacts(conn, n=500):
    """Génère n contacts CRM"""
    print(f"\nGénération de {n} contacts CRM...")
    cursor = conn.cursor()
    
    contacts = []
    for i in range(n):
        if i % 100 == 0:
            print(f"   ... {i}/{n} contacts générés")
        
        converted = random.random() > 0.6
        created_at = fake.date_time_between(start_date='-1y', end_date='now')
        
        contact = (
            fake.email(),
            fake.first_name(),
            fake.last_name(),
            random.choice(SOURCES),
            f"CAMP_{random.randint(1000, 9999)}",
            created_at,
            converted,
            fake.date_time_between(start_date=created_at, end_date='now') if converted else None
        )
        contacts.append(contact)
    
    query = """
        INSERT INTO crm_contacts (email, first_name, last_name, source, campaign_id, created_at, converted, converted_at)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
    """
    
    try:
        execute_batch(cursor, query, contacts, page_size=100)
        conn.commit()
        print(f"{n} contacts CRM créés")
    except Exception as e:
        print(f"Erreur lors de la création des contacts : {e}")
        conn.rollback()

def main():
    """Point d'entrée principal"""
    print("="*70)
    print("GENERATION DE DONNEES SHOPSTREAM")
    print("="*70)
    
    conn = get_connection()
    
    try:
        # Génération dans l'ordre (à cause des clés étrangères)
        generate_users(conn, n=1000)
        generate_products(conn, n=200)
        generate_orders(conn, n=5000)
        generate_events(conn, n=10000)
        generate_crm_contacts(conn, n=500)
        
        print("\n" + "="*70)
        print("GENERATION TERMINEE AVEC SUCCES")
        print("="*70)
        print("\nRécapitulatif :")
        cursor = conn.cursor()
        cursor.execute("SELECT 'users' AS table_name, COUNT(*) FROM users")
        print(f"   - Users : {cursor.fetchone()[1]}")
        cursor.execute("SELECT COUNT(*) FROM products")
        print(f"   - Products : {cursor.fetchone()[0]}")
        cursor.execute("SELECT COUNT(*) FROM orders")
        print(f"   - Orders : {cursor.fetchone()[0]}")
        cursor.execute("SELECT COUNT(*) FROM order_items")
        print(f"   - Order Items : {cursor.fetchone()[0]}")
        cursor.execute("SELECT COUNT(*) FROM events")
        print(f"   - Events : {cursor.fetchone()[0]}")
        cursor.execute("SELECT COUNT(*) FROM crm_contacts")
        print(f"   - CRM Contacts : {cursor.fetchone()[0]}")
        
    except Exception as e:
        print(f"\nERREUR GLOBALE : {e}")
        conn.rollback()
    finally:
        conn.close()
        print("\nConnexion fermée.")

if __name__ == "__main__":
    main()
</code></pre>

                <div class="warning-box">
                    <strong>IMPORTANT</strong><br>
                    Modifiez la ligne <code>DB_CONFIG</code> avec VOTRE mot de passe PostgreSQL :
                    <pre><code>'password': 'postgres123'  # METTEZ VOTRE MOT DE PASSE ICI</code></pre>
                </div>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">5</span>Exécution du script de génération</h4>
                
                <ol>
                    <li>Ouvrez une invite de commande (CMD ou Terminal)</li>
                    <li>Naviguez vers votre dossier scripts :
                        <p><em>Windows :</em></p>
                        <div class="command-line">
                            <code>cd C:\Users\VotreNom\ShopStreamTP\scripts</code>
                        </div>
                        <p><em>macOS/Linux :</em></p>
                        <div class="command-line">
                            <code>cd ~/ShopStreamTP/scripts</code>
                        </div>
                    </li>
                    <li>Exécutez le script :
                        <div class="command-line">
                            <code>python generate_data.py</code>
                        </div>
                    </li>
                    <li>Le script démarre.  Vous voyez :
                        <pre><code>======================================================================
GENERATION DE DONNEES SHOPSTREAM
======================================================================
Connexion à PostgreSQL...
Connexion réussie

Génération de 1000 utilisateurs...
   ... 0/1000 utilisateurs générés
   ... 100/1000 utilisateurs générés
   ...</code></pre>
                    </li>
                    <li>Patientez (durée : 2-5 minutes selon votre machine)</li>
                    <li>À la fin, vous devez voir :
                        <pre><code>======================================================================
GENERATION TERMINEE AVEC SUCCES
======================================================================

Récapitulatif :
   - Users : 1000
   - Products : 200
   - Orders : 5000
   - Order Items : ~15000
   - Events : 10000
   - CRM Contacts : 500

Connexion fermée. </code></pre>
                    </li>
                    <li>Vos données sont générées</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">6</span>Vérification des données dans pgAdmin</h4>
                
                <ol>
                    <li>Retournez dans pgAdmin</li>
                    <li>Dans le Query Tool (fenêtre SQL), tapez :
                        <pre><code>SELECT * FROM users LIMIT 10;</code></pre>
                    </li>
                    <li><div class="click-instruction">Cliquez sur le bouton <strong>"Execute"</strong></div></li>
                    <li>Vous voyez 10 utilisateurs s'afficher dans le tableau en bas</li>
                    <li>Testez aussi avec :
                        <pre><code>SELECT * FROM orders LIMIT 10;
SELECT * FROM products LIMIT 10;</code></pre>
                    </li>
                    <li>Les données sont bien présentes dans votre base</li>
                </ol>
            </div>
            
            <!-- Section 6 - Amazon S3 -->
            <h2 id="section6">6. Configuration d'Amazon S3</h2>

            <div class="pipeline-context">
                <h3>Contexte dans le pipeline</h3>
                <p>Nous passons maintenant à la deuxième étape : la mise en place du Data Lake.  Les données de PostgreSQL vont être exportées vers Amazon S3 qui servira de zone de stockage brut (raw).</p>
                <div class="pipeline-steps">
                    <div class="pipeline-step">PostgreSQL (OLTP)</div>
                    <div class="pipeline-step active">S3 (Data Lake)</div>
                    <div class="pipeline-step">Snowflake (DWH)</div>
                    <div class="pipeline-step">dbt (Transform)</div>
                    <div class="pipeline-step">Airflow (Orchestration)</div>
                    <div class="pipeline-step">Power BI (BI)</div>
                </div>
            </div>

            <p>Continuez avec les sections S3, Snowflake, dbt, Airflow et Power BI en suivant le même format ultra-détaillé... </p>

         

            <!-- Continuation Section 6 -->
            <h3>6. 1 Création d'un compte AWS</h3>
            
            <div class="step-box">
                <h4><span class="step-number">1</span>Inscription sur AWS</h4>
                
                <ol>
                    <li>Allez sur <a href="https://aws.amazon. com/" target="_blank">https://aws. amazon.com/</a></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Créer un compte AWS"</strong></div></li>
                    <li>Remplissez le formulaire :
                        <ul>
                            <li>Adresse email</li>
                            <li>Nom du compte (exemple : <code>ShopStream-TP</code>)</li>
                        </ul>
                    </li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Continuer"</strong></div></li>
                    <li>Vérifiez votre email et entrez le code de vérification</li>
                    <li>Créez un mot de passe sécurisé</li>
                    <li>Renseignez vos informations personnelles</li>
                    <li><strong>Carte bancaire requise</strong> (mais vous resterez dans l'offre gratuite)</li>
                    <li>Choisissez le plan <strong>"Gratuit"</strong></li>
                    <li>Confirmez votre inscription</li>
                    <li>Votre compte AWS est créé</li>
                </ol>
                
                <div class="info-box">
                    <strong>Offre gratuite AWS (Free Tier)</strong><br>
                    AWS offre 12 mois gratuits incluant :
                    <ul>
                        <li>5 Go de stockage S3</li>
                        <li>20 000 requêtes GET</li>
                        <li>2 000 requêtes PUT</li>
                    </ul>
                    Pour ce TP, vous resterez largement dans ces limites.
                </div>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">2</span>Connexion à la console AWS</h4>
                
                <ol>
                    <li>Allez sur <a href="https://console.aws.amazon.com/" target="_blank">https://console. aws.amazon.com/</a></li>
                    <li>Connectez-vous avec votre email et mot de passe</li>
                    <li>Vous arrivez sur la <strong>Console AWS</strong></li>
                    <li>En haut à droite, vérifiez que la région sélectionnée est <strong>"Europe (Paris) eu-west-3"</strong> ou <strong>"Europe (Ireland) eu-west-1"</strong></li>
                    <li><div class="click-instruction">Si ce n'est pas le cas, cliquez sur le nom de la région en haut à droite et sélectionnez <strong>"Europe (Paris)"</strong></div></li>
                    <li>Vous êtes maintenant dans la console AWS</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">3</span>Création d'un bucket S3</h4>
                
                <ol>
                    <li>Dans la barre de recherche en haut de la console AWS, tapez : <code>S3</code></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"S3"</strong> dans les résultats</div></li>
                    <li>Vous arrivez sur la page S3</li>
                    <li><div class="click-instruction">Cliquez sur le bouton orange <strong>"Créer un compartiment"</strong> (ou "Create bucket")</div></li>
                    <li>Remplissez le formulaire :
                        <ul>
                            <li><strong>Nom du compartiment</strong> : <code>shopstream-datalake-votreprenom</code>
                                <br><small>Le nom doit être UNIQUE dans tout AWS.  Ajoutez votre prénom ou des chiffres</small>
                            </li>
                            <li><strong>Région AWS</strong> : <code>Europe (Paris) eu-west-3</code> (doit être la même que votre région actuelle)</li>
                        </ul>
                    </li>
                    <li>Section <strong>"Paramètres de blocage de l'accès public"</strong> :
                        <ul>
                            <li>Laissez TOUTES les cases cochées (sécurité par défaut)</li>
                        </ul>
                    </li>
                    <li>Section <strong>"Chiffrement par défaut"</strong> :
                        <ul>
                            <li>Sélectionnez <strong>"Activer"</strong></li>
                            <li>Type de clé : <strong>"Clé gérée par Amazon S3 (SSE-S3)"</strong></li>
                        </ul>
                    </li>
                    <li>Laissez les autres options par défaut</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Créer un compartiment"</strong> en bas de la page</div></li>
                    <li>Vous voyez un message vert : <code>Compartiment "shopstream-datalake-votreprenom" créé avec succès</code></li>
                    <li>Votre bucket S3 est créé</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">4</span>Création de la structure de dossiers dans S3</h4>
                
                <ol>
                    <li>Dans la liste des buckets, <div class="click-instruction">cliquez sur le nom de votre bucket <strong>"shopstream-datalake-votreprenom"</strong></div></li>
                    <li>Vous êtes maintenant à l'intérieur du bucket (vide pour l'instant)</li>
                    <li><div class="click-instruction">Cliquez sur le bouton <strong>"Créer un dossier"</strong></div></li>
                    <li>Nom du dossier : <code>raw</code></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Créer un dossier"</strong></div></li>
                    <li>Le dossier <code>raw/</code> apparaît dans votre bucket</li>
                    <li><div class="click-instruction">Cliquez sur le dossier <strong>"raw"</strong> pour entrer dedans</div></li>
                    <li>Créez les sous-dossiers suivants (répétez les étapes ci-dessus) :
                        <ul>
                            <li><code>postgres</code></li>
                            <li><code>events</code></li>
                            <li><code>crm</code></li>
                        </ul>
                    </li>
                    <li><div class="click-instruction">Entrez dans le dossier <strong>"postgres"</strong></div></li>
                    <li>Créez les sous-dossiers :
                        <ul>
                            <li><code>users</code></li>
                            <li><code>products</code></li>
                            <li><code>orders</code></li>
                            <li><code>order_items</code></li>
                        </ul>
                    </li>
                    <li>Votre structure de dossiers est prête</li>
                </ol>
                
                <p><strong>Structure finale :</strong></p>
                <pre><code>shopstream-datalake-votreprenom/
└── raw/
    ├── postgres/
    │   ├── users/
    │   ├── products/
    │   ├── orders/
    │   └── order_items/
    ├── events/
    └── crm/
</code></pre>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">5</span>Création d'un utilisateur IAM avec accès S3</h4>
                
                <p><strong>Pourquoi ? </strong> Pour permettre à Python (et plus tard à Snowflake) d'accéder à S3, nous avons besoin de clés d'accès.</p>
                
                <ol>
                    <li>Dans la barre de recherche AWS, tapez : <code>IAM</code></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"IAM"</strong></div></li>
                    <li>Dans le menu latéral gauche, <div class="click-instruction">cliquez sur <strong>"Utilisateurs"</strong></div></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Créer un utilisateur"</strong></div></li>
                    <li>Nom d'utilisateur : <code>shopstream-s3-user</code></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Suivant"</strong></div></li>
                    <li>Section <strong>"Définir les autorisations"</strong> :
                        <ul>
                            <li>Sélectionnez <strong>"Attacher directement des stratégies"</strong></li>
                            <li>Dans la barre de recherche, tapez : <code>S3Full</code></li>
                            <li>Cochez <strong>"AmazonS3FullAccess"</strong></li>
                        </ul>
                    </li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Suivant"</strong></div></li>
                    <li>Vérifiez le résumé</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Créer un utilisateur"</strong></div></li>
                    <li>L'utilisateur est créé</li>
                    <li><div class="click-instruction">Cliquez sur le nom de l'utilisateur <strong>"shopstream-s3-user"</strong></div></li>
                    <li>Allez dans l'onglet <strong>"Informations d'identification de sécurité"</strong></li>
                    <li>Descendez jusqu'à la section <strong>"Clés d'accès"</strong></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Créer une clé d'accès"</strong></div></li>
                    <li>Cas d'utilisation : <strong>"Application s'exécutant en dehors d'AWS"</strong></li>
                    <li>Cochez la case de confirmation</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Suivant"</strong></div></li>
                    <li>Étiquette (optionnel) : <code>ShopStream TP</code></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Créer une clé d'accès"</strong></div></li>
                    <li><strong>IMPORTANT</strong> : Vous voyez maintenant :
                        <ul>
                            <li><strong>Clé d'accès</strong> (Access Key ID) : exemple <code>AKIAIOSFODNN7EXAMPLE</code></li>
                            <li><strong>Clé d'accès secrète</strong> (Secret Access Key) : exemple <code>wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY</code></li>
                        </ul>
                    </li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Télécharger le fichier . csv"</strong></div></li>
                    <li><strong>Sauvegardez ce fichier précieusement</strong> Vous ne pourrez plus voir la clé secrète après avoir fermé cette fenêtre</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Terminé"</strong></div></li>
                    <li>Vos clés d'accès AWS sont créées</li>
                </ol>
                
                <div class="warning-box">
                    <strong>SECURITE IMPORTANTE</strong><br>
                    <ul>
                        <li>Ne partagez JAMAIS vos clés d'accès AWS</li>
                        <li>Ne les commitez JAMAIS dans Git</li>
                        <li>Ne les publiez JAMAIS en ligne</li>
                        <li>Stockez-les dans un gestionnaire de mots de passe</li>
                    </ul>
                </div>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">6</span>Configuration d'AWS CLI</h4>
                
                <p><strong>Installation d'AWS CLI :</strong></p>
                
                <p><strong>Windows :</strong></p>
                <ol>
                    <li>Téléchargez AWS CLI : <a href="https://awscli.amazonaws.com/AWSCLIV2.msi" target="_blank">https://awscli.amazonaws.com/AWSCLIV2.msi</a></li>
                    <li>Exécutez le fichier MSI téléchargé</li>
                    <li>Suivez l'assistant d'installation (tout laisser par défaut)</li>
                    <li>Fermez et rouvrez votre invite de commande</li>
                </ol>
                
                <p><strong>macOS :</strong></p>
                <div class="command-line">
                    <code>brew install awscli</code>
                </div>
                
                <p><strong>Linux :</strong></p>
                <div class="command-line">
                    <code>curl "https://awscli.amazonaws. com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"</code><br>
                    <code>unzip awscliv2. zip</code><br>
                    <code>sudo ./aws/install</code>
                </div>
                
                <p><strong>Configuration d'AWS CLI :</strong></p>
                <ol>
                    <li>Ouvrez une invite de commande</li>
                    <li>Tapez :
                        <div class="command-line">
                            <code>aws configure</code>
                        </div>
                    </li>
                    <li>Répondez aux questions :
                        <pre><code>AWS Access Key ID [None]: <strong>COLLEZ_VOTRE_ACCESS_KEY_ID_ICI</strong>
AWS Secret Access Key [None]: <strong>COLLEZ_VOTRE_SECRET_ACCESS_KEY_ICI</strong>
Default region name [None]: <strong>eu-west-3</strong>
Default output format [None]: <strong>json</strong></code></pre>
                    </li>
                    <li>AWS CLI est configuré</li>
                    <li>Testez avec :
                        <div class="command-line">
                            <code>aws s3 ls</code>
                        </div>
                    </li>
                    <li>Vous devez voir votre bucket s'afficher :
                        <pre><code>2025-11-27 10:30:45 shopstream-datalake-votreprenom</code></pre>
                    </li>
                    <li>La connexion AWS fonctionne</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">7</span>Script Python pour exporter PostgreSQL vers S3</h4>
                
                <div class="file-path">
                    <strong>Emplacement exact du fichier à créer :</strong><br>
                    Windows : <code>C:\Users\VotreNom\ShopStreamTP\scripts\export_to_s3.py</code><br>
                    macOS/Linux : <code>~/ShopStreamTP/scripts/export_to_s3.py</code>
                </div>
                
                <ol>
                    <li>Installez la bibliothèque boto3 (SDK AWS pour Python) :
                        <div class="command-line">
                            <code>pip install boto3</code>
                        </div>
                    </li>
                    <li>Ouvrez votre éditeur de texte (VS Code, Notepad++, etc.)</li>
                    <li>Créez un nouveau fichier</li>
                    <li>Copiez-collez le script suivant :</li>
                </ol>

<pre><code class="language-python">"""
export_to_s3.py
Exporte les tables PostgreSQL vers S3 au format CSV
Emplacement : ShopStreamTP/scripts/export_to_s3.py
"""

import os
from datetime import datetime
import psycopg2
import pandas as pd
import boto3
from io import StringIO

# CONFIGURATION A MODIFIER
DB_CONFIG = {
    'host': 'localhost',
    'database': 'shopstream',
    'user': 'postgres',
    'password': 'postgres123'  # VOTRE mot de passe PostgreSQL
}

S3_CONFIG = {
    'bucket': 'shopstream-datalake-votreprenom',  # VOTRE nom de bucket S3
    'region': 'eu-west-3'
}

# Tables à exporter
TABLES = ['users', 'products', 'orders', 'order_items', 'crm_contacts']

def get_db_connection():
    """Connexion PostgreSQL"""
    print("Connexion à PostgreSQL...")
    try:
        conn = psycopg2. connect(**DB_CONFIG)
        print("Connexion PostgreSQL réussie")
        return conn
    except Exception as e:
        print(f"Erreur de connexion PostgreSQL : {e}")
        exit(1)

def get_s3_client():
    """Client S3"""
    print("Connexion à AWS S3...")
    try:
        s3_client = boto3.client('s3', region_name=S3_CONFIG['region'])
        # Test de connexion
        s3_client. head_bucket(Bucket=S3_CONFIG['bucket'])
        print("Connexion S3 réussie")
        return s3_client
    except Exception as e:
        print(f"Erreur de connexion S3 : {e}")
        print("Vérifiez votre nom de bucket et vos credentials AWS")
        exit(1)

def export_table_to_s3(table_name, date_partition):
    """
    Exporte une table PostgreSQL vers S3 au format CSV
    
    Args:
        table_name: Nom de la table PostgreSQL
        date_partition: Date de partition (format YYYY-MM-DD)
    """
    print(f"\nExport de la table '{table_name}'...")
    
    # Connexion DB
    conn = get_db_connection()
    
    # Lecture de la table dans un DataFrame Pandas
    query = f"SELECT * FROM {table_name}"
    try:
        df = pd.read_sql(query, conn)
        print(f"   {len(df)} lignes extraites de PostgreSQL")
    except Exception as e:
        print(f"   Erreur lecture table : {e}")
        conn.close()
        return
    
    conn.close()
    
    if df.empty:
        print(f"   Table '{table_name}' vide, pas d'export")
        return
    
    # Conversion en CSV (en mémoire)
    csv_buffer = StringIO()
    df. to_csv(csv_buffer, index=False)
    
    # Chemin S3
    date_folder = date_partition
    s3_key = f"raw/postgres/{table_name}/{date_folder}/{table_name}_{date_partition. replace('-', '')}.csv"
    
    # Upload vers S3
    s3_client = get_s3_client()
    try:
        s3_client. put_object(
            Bucket=S3_CONFIG['bucket'],
            Key=s3_key,
            Body=csv_buffer.getvalue()
        )
        print(f"   Uploadé vers s3://{S3_CONFIG['bucket']}/{s3_key}")
    except Exception as e:
        print(f"   Erreur upload S3 : {e}")

def export_events_to_s3(date_partition):
    """Exporte les événements au format JSON"""
    print(f"\nExport de la table 'events'...")
    
    conn = get_db_connection()
    
    query = "SELECT id, user_id, event_type, event_ts, metadata FROM events"
    try:
        df = pd.read_sql(query, conn)
        print(f"   {len(df)} lignes extraites de PostgreSQL")
    except Exception as e:
        print(f"   Erreur lecture events : {e}")
        conn. close()
        return
    
    conn.close()
    
    if df.empty:
        print(f"   Table 'events' vide, pas d'export")
        return
    
    # Conversion en JSON
    json_data = df.to_json(orient='records', date_format='iso')
    
    # Chemin S3
    s3_key = f"raw/events/{date_partition}/events_{date_partition.replace('-', '')}.json"
    
    # Upload
    s3_client = get_s3_client()
    try:
        s3_client. put_object(
            Bucket=S3_CONFIG['bucket'],
            Key=s3_key,
            Body=json_data
        )
        print(f"   Uploadé vers s3://{S3_CONFIG['bucket']}/{s3_key}")
    except Exception as e:
        print(f"   Erreur upload S3 : {e}")

def main():
    """Point d'entrée principal"""
    print("="*70)
    print("EXPORT POSTGRESQL vers S3")
    print("="*70)
    
    # Date du jour (ou passée en argument)
    today = datetime.now().strftime('%Y-%m-%d')
    print(f"\nDate de partition : {today}")
    
    try:
        # Export des tables relationnelles
        for table in TABLES:
            export_table_to_s3(table, today)
        
        # Export des événements (JSON)
        export_events_to_s3(today)
        
        print("\n" + "="*70)
        print("EXPORT TERMINE AVEC SUCCES")
        print("="*70)
        print(f"\nVérifiez dans S3 : https://s3.console.aws. amazon.com/s3/buckets/{S3_CONFIG['bucket']}")
        
    except Exception as e:
        print(f"\nERREUR GLOBALE : {e}")

if __name__ == "__main__":
    main()
</code></pre>

                <ol start="5">
                    <li>Sauvegardez le fichier sous le nom <code>export_to_s3.py</code> dans le dossier <code>ShopStreamTP/scripts/</code></li>
                </ol>
                
                <div class="warning-box">
                    <strong>IMPORTANT - Modifiez ces lignes :</strong>
                    <ul>
                        <li>Ligne 13 : <code>'password': 'postgres123'</code> - Mettez votre mot de passe PostgreSQL</li>
                        <li>Ligne 17 : <code>'bucket': 'shopstream-datalake-votreprenom'</code> - Mettez votre nom de bucket S3</li>
                        <li>Ligne 18 : <code>'region': 'eu-west-3'</code> - Mettez votre région AWS</li>
                    </ul>
                </div>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">8</span>Exécution de l'export vers S3</h4>
                
                <ol>
                    <li>Dans votre invite de commande, naviguez vers le dossier scripts :
                        <p><em>Windows :</em></p>
                        <div class="command-line">
                            <code>cd C:\Users\VotreNom\ShopStreamTP\scripts</code>
                        </div>
                        <p><em>macOS/Linux :</em></p>
                        <div class="command-line">
                            <code>cd ~/ShopStreamTP/scripts</code>
                        </div>
                    </li>
                    <li>Exécutez le script :
                        <div class="command-line">
                            <code>python export_to_s3.py</code>
                        </div>
                    </li>
                    <li>Le script s'exécute.  Vous voyez :
                        <pre><code>======================================================================
EXPORT POSTGRESQL vers S3
======================================================================

Date de partition : 2025-11-27

Export de la table 'users'... 
Connexion à PostgreSQL... 
Connexion PostgreSQL réussie
   1000 lignes extraites de PostgreSQL
Connexion à AWS S3... 
Connexion S3 réussie
   Uploadé vers s3://shopstream-datalake-votreprenom/raw/postgres/users/2025-11-27/users_20251127.csv

...  (autres tables)

======================================================================
EXPORT TERMINE AVEC SUCCES
======================================================================</code></pre>
                    </li>
                    <li>Vos données sont maintenant dans S3</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">9</span>Vérification des fichiers dans S3</h4>
                
                <ol>
                    <li>Retournez dans la console AWS S3</li>
                    <li>Naviguez dans votre bucket : <code>shopstream-datalake-votreprenom > raw > postgres > users > 2025-11-27</code></li>
                    <li>Vous devez voir le fichier : <code>users_20251127.csv</code></li>
                    <li><div class="click-instruction">Cliquez sur le fichier</div></li>
                    <li>Vous voyez les détails du fichier (taille, date de création, etc.)</li>
                    <li><div class="click-instruction">Cliquez sur le bouton <strong>"Télécharger"</strong></div></li>
                    <li>Ouvrez le fichier CSV téléchargé avec Excel ou un éditeur de texte</li>
                    <li>Vous voyez vos données d'utilisateurs au format CSV</li>
                </ol>
            </div>
            
            <!-- Section 7 - Snowflake -->
            <h2 id="section7">7. Configuration de Snowflake</h2>

            <div class="pipeline-context">
                <h3>Contexte dans le pipeline</h3>
                <p>Nous passons maintenant à la troisième étape : Snowflake, notre Data Warehouse cloud. Les données stockées dans S3 (Data Lake) vont être chargées dans Snowflake pour permettre des analyses rapides et complexes.</p>
                <div class="pipeline-steps">
                    <div class="pipeline-step">PostgreSQL (OLTP)</div>
                    <div class="pipeline-step">S3 (Data Lake)</div>
                    <div class="pipeline-step active">Snowflake (DWH)</div>
                    <div class="pipeline-step">dbt (Transform)</div>
                    <div class="pipeline-step">Airflow (Orchestration)</div>
                    <div class="pipeline-step">Power BI (BI)</div>
                </div>
            </div>
            
            <h3>7.1 Création d'un compte Snowflake (essai gratuit 30 jours)</h3>
            
            <div class="step-box">
                <h4><span class="step-number">1</span>Inscription sur Snowflake</h4>
                
                <ol>
                    <li>Allez sur <a href="https://signup.snowflake.com/" target="_blank">https://signup.snowflake.com/</a></li>
                    <li>Remplissez le formulaire :
                        <ul>
                            <li><strong>First Name</strong> : Votre prénom</li>
                            <li><strong>Last Name</strong> : Votre nom</li>
                            <li><strong>Email</strong> : Votre email</li>
                            <li><strong>Company</strong> : <code>Student - TP ShopStream</code></li>
                            <li><strong>Country</strong> : France</li>
                        </ul>
                    </li>
                    <li><strong>Snowflake Edition</strong> : Sélectionnez <strong>"Standard"</strong></li>
                    <li><strong>Cloud Provider</strong> : Sélectionnez <strong>"Amazon Web Services"</strong></li>
                    <li><strong>Region</strong> : Sélectionnez <strong>"Europe (Paris) - eu-west-3"</strong> (même région que votre bucket S3)</li>
                    <li>Acceptez les conditions d'utilisation</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"GET STARTED"</strong></div></li>
                    <li>Vous recevez un email de confirmation</li>
                    <li>Ouvrez votre boîte mail et <div class="click-instruction">cliquez sur le lien d'activation</div></li>
                    <li>Définissez un nom d'utilisateur et un mot de passe sécurisé</li>
                    <li>Votre compte Snowflake est créé</li>
                </ol>
                
                <div class="info-box">
                    <strong>Essai gratuit Snowflake</strong><br>
                    Snowflake offre 30 jours d'essai avec $400 de crédits gratuits.  
                    Pour ce TP, vous consommerez moins de $10 de crédits.
                </div>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">2</span>Première connexion à Snowflake</h4>
                
                <ol>
                    <li>Après activation, vous êtes redirigé vers l'interface Snowflake</li>
                    <li>L'URL ressemble à : <code>https://abc12345.eu-west-3.aws.snowflakecomputing.com/</code></li>
                    <li><strong>Notez cette URL</strong> C'est votre <strong>Account Locator</strong></li>
                    <li>Vous arrivez sur le <strong>Snowsight</strong> (nouvelle interface Snowflake)</li>
                    <li>Un tutoriel s'affiche.  <div class="click-instruction">Cliquez sur <strong>"Skip"</strong> ou fermez-le</div></li>
                    <li>Vous êtes connecté à Snowflake</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">3</span>Création de la base de données ShopStream</h4>
                
                <ol>
                    <li>Dans le menu latéral gauche, <div class="click-instruction">cliquez sur <strong>"Data" > "Databases"</strong></div></li>
                    <li>En haut à droite, <div class="click-instruction">cliquez sur le bouton bleu <strong>"+ Database"</strong></div></li>
                    <li>Une fenêtre s'ouvre</li>
                    <li>Nom : <code>SHOPSTREAM_DWH</code></li>
                    <li>Laissez les autres options par défaut</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Create"</strong></div></li>
                    <li>La base de données <code>SHOPSTREAM_DWH</code> apparaît dans la liste</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">4</span>Création des schémas (RAW, STAGING, CORE, MARTS)</h4>
                
                <ol>
                    <li>Dans le menu gauche, <div class="click-instruction">cliquez sur <strong>"Projects" > "Worksheets"</strong></div></li>
                    <li>En haut à droite, <div class="click-instruction">cliquez sur <strong>"+ Worksheet"</strong></div></li>
                    <li>Un éditeur SQL vierge s'ouvre</li>
                    <li>Copiez-collez le script SQL suivant :</li>
                </ol>

<pre><code class="language-sql">-- ============================================
-- Configuration initiale Snowflake pour ShopStream
-- ============================================

-- Utilisation de la base de données
USE DATABASE SHOPSTREAM_DWH;

-- Création des schémas
CREATE SCHEMA IF NOT EXISTS RAW;
CREATE SCHEMA IF NOT EXISTS STAGING;
CREATE SCHEMA IF NOT EXISTS CORE;
CREATE SCHEMA IF NOT EXISTS MARTS;

-- Vérification
SHOW SCHEMAS;

-- Message de confirmation
SELECT 'Configuration initiale terminée' AS message;
</code></pre>

                <ol start="5">
                    <li><div class="click-instruction">Cliquez sur le bouton bleu <strong>"Run"</strong> en haut à droite</div></li>
                    <li>Le script s'exécute (quelques secondes)</li>
                    <li>En bas, vous voyez les résultats :
                        <ul>
                            <li>La liste des schémas créés</li>
                            <li>Le message : <code>Configuration initiale terminée</code></li>
                        </ul>
                    </li>
                    <li>Vos schémas sont créés</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">5</span>Création des Virtual Warehouses</h4>
                
                <ol>
                    <li>Dans le même Worksheet, effacez le contenu précédent</li>
                    <li>Copiez-collez le script suivant :</li>
                </ol>

<pre><code class="language-sql">-- ============================================
-- Création des Virtual Warehouses
-- ============================================

-- Warehouse pour les chargements
CREATE WAREHOUSE IF NOT EXISTS LOADING_WH
    WITH WAREHOUSE_SIZE = 'XSMALL'
    AUTO_SUSPEND = 60
    AUTO_RESUME = TRUE
    INITIALLY_SUSPENDED = TRUE
    COMMENT = 'Warehouse pour charger les données depuis S3';

-- Warehouse pour les transformations dbt
CREATE WAREHOUSE IF NOT EXISTS TRANSFORM_WH
    WITH WAREHOUSE_SIZE = 'SMALL'
    AUTO_SUSPEND = 60
    AUTO_RESUME = TRUE
    INITIALLY_SUSPENDED = TRUE
    COMMENT = 'Warehouse pour les transformations dbt';

-- Warehouse pour la BI
CREATE WAREHOUSE IF NOT EXISTS BI_WH
    WITH WAREHOUSE_SIZE = 'XSMALL'
    AUTO_SUSPEND = 120
    AUTO_RESUME = TRUE
    INITIALLY_SUSPENDED = TRUE
    COMMENT = 'Warehouse pour les requêtes BI';

-- Vérification
SHOW WAREHOUSES;

SELECT 'Warehouses créés avec succès' AS message;
</code></pre>

                <ol start="3">
                    <li><div class="click-instruction">Cliquez sur <strong>"Run"</strong></div></li>
                    <li>Vous voyez la liste des 3 warehouses créés</li>
                    <li>Vos Virtual Warehouses sont créés</li>
                </ol>
                
                <div class="info-box">
                    <strong>Tailles de Warehouse et coûts</strong><br>
                    <ul>
                        <li><strong>XSMALL</strong> : 1 crédit/heure (environ $2/heure, mais auto-suspend après 60 secondes)</li>
                        <li><strong>SMALL</strong> : 2 crédits/heure</li>
                        <li>Pour ce TP, coût estimé : moins de $5</li>
                    </ul>
                </div>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">6</span>Création d'une Storage Integration (connexion S3 - Snowflake)</h4>
                
                <p><strong>Étape A : Création d'un rôle IAM pour Snowflake</strong></p>
                
                <ol>
                    <li>Retournez dans la console AWS</li>
                    <li>Allez dans <strong>IAM > Rôles</strong></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Créer un rôle"</strong></div></li>
                    <li>Type d'entité de confiance : <strong>"Compte AWS"</strong></li>
                    <li><strong>Un autre compte AWS</strong> : Cochez cette option</li>
                    <li>ID du compte : Mettez un ID temporaire : <code>123456789012</code> (nous le modifierons plus tard)</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Suivant"</strong></div></li>
                    <li>Recherchez et sélectionnez : <strong>"AmazonS3FullAccess"</strong></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Suivant"</strong></div></li>
                    <li>Nom du rôle : <code>snowflake-s3-integration-role</code></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Créer un rôle"</strong></div></li>
                    <li>Le rôle est créé.  <div class="click-instruction">Cliquez dessus</div></li>
                    <li>Copiez l'<strong>ARN du rôle</strong> (ressemble à : <code>arn:aws:iam::123456789012:role/snowflake-s3-integration-role</code>)</li>
                    <li><strong>Notez cet ARN quelque part</strong></li>
                </ol>
                
                <p><strong>Étape B : Création de la Storage Integration dans Snowflake</strong></p>
                
                <ol>
                    <li>Retournez dans votre Worksheet Snowflake</li>
                    <li>Copiez-collez le script suivant (MODIFIEZ l'ARN ET le nom du bucket) :</li>
                </ol>

<pre><code class="language-sql">-- ============================================
-- Création de la Storage Integration (S3 - Snowflake)
-- ============================================

USE ROLE ACCOUNTADMIN;

CREATE OR REPLACE STORAGE INTEGRATION s3_integration
    TYPE = EXTERNAL_STAGE
    STORAGE_PROVIDER = S3
    ENABLED = TRUE
    STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::123456789012:role/snowflake-s3-integration-role'  -- VOTRE ARN ICI
    STORAGE_ALLOWED_LOCATIONS = ('s3://shopstream-datalake-votreprenom/raw/');  -- VOTRE BUCKET ICI

-- Récupération des informations pour AWS
DESC INTEGRATION s3_integration;
</code></pre>

                <ol start="3">
                    <li><div class="click-instruction">Cliquez sur <strong>"Run"</strong></div></li>
                    <li>Dans les résultats, cherchez les lignes :
                        <ul>
                            <li><code>STORAGE_AWS_IAM_USER_ARN</code> : exemple <code>arn:aws:iam::123456789:user/abc-defg</code></li>
                            <li><code>STORAGE_AWS_EXTERNAL_ID</code> : exemple <code>ABC123_SFCRole=1_xyz</code></li>
                        </ul>
                    </li>
                    <li><strong>Notez ces deux valeurs</strong></li>
                </ol>
                
                <p><strong>Étape C : Configuration de la relation de confiance dans AWS</strong></p>
                
                <ol>
                    <li>Retournez dans AWS IAM > Rôles</li>
                    <li>Cliquez sur le rôle <code>snowflake-s3-integration-role</code></li>
                    <li>Allez dans l'onglet <strong>"Relations de confiance"</strong></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Modifier la stratégie de relation de confiance"</strong></div></li>
                    <li>Remplacez TOUT le contenu par (MODIFIEZ avec VOS valeurs Snowflake) :</li>
                </ol>

<pre><code class="language-json">{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::123456789:user/abc-defg"
      },
      "Action": "sts:AssumeRole",
      "Condition": {
        "StringEquals": {
          "sts:ExternalId": "ABC123_SFCRole=1_xyz"
        }
      }
    }
  ]
}
</code></pre>

                <ol start="6">
                    <li>Remplacez <code>arn:aws:iam::123456789:user/abc-defg</code> par VOTRE <code>STORAGE_AWS_IAM_USER_ARN</code></li>
                    <li>Remplacez <code>ABC123_SFCRole=1_xyz</code> par VOTRE <code>STORAGE_AWS_EXTERNAL_ID</code></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Mettre à jour la stratégie"</strong></div></li>
                    <li>La relation de confiance est configurée</li>
                </ol>
                
                <div class="warning-box">
                    <strong>ATTENTION</strong><br>
                    Cette étape est la plus technique du TP. Si vous rencontrez des erreurs :
                    <ul>
                        <li>Vérifiez que vous avez bien copié les ARN et External ID</li>
                        <li>Vérifiez que votre bucket S3 et votre compte Snowflake sont dans la même région</li>
                        <li>Consultez la documentation officielle : <a href="https://docs. snowflake.com/en/user-guide/data-load-s3-config-storage-integration. html" target="_blank">Snowflake S3 Integration</a></li>
                    </ul>
                </div>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">7</span>Création du Stage externe (pointant vers S3)</h4>
                
                <ol>
                    <li>Dans votre Worksheet Snowflake, copiez-collez :</li>
                </ol>

<pre><code class="language-sql">-- ============================================
-- Création du Stage externe (S3)
-- ============================================

USE DATABASE SHOPSTREAM_DWH;
USE SCHEMA RAW;
USE WAREHOUSE LOADING_WH;

-- Création du Stage
CREATE OR REPLACE STAGE s3_raw_stage
    STORAGE_INTEGRATION = s3_integration
    URL = 's3://shopstream-datalake-votreprenom/raw/'  -- VOTRE BUCKET ICI
    FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY = '"' SKIP_HEADER = 1);

-- Test du Stage : liste des fichiers
LIST @s3_raw_stage/postgres/users/;

-- Si vous voyez vos fichiers CSV, c'est réussi
</code></pre>

                <ol start="2">
                    <li><div class="click-instruction">Cliquez sur <strong>"Run"</strong></div></li>
                    <li>Dans les résultats, vous devez voir votre fichier CSV :
                        <pre><code>s3://shopstream-datalake-votreprenom/raw/postgres/users/2025-11-27/users_20251127.csv</code></pre>
                    </li>
                    <li>Snowflake peut maintenant lire vos fichiers S3</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">8</span>Création des tables de STAGING</h4>
                
                <ol>
                    <li>Dans votre Worksheet, copiez-collez :</li>
                </ol>

<pre><code class="language-sql">-- ============================================
-- Création des tables de STAGING
-- ============================================

USE SCHEMA STAGING;

-- Table STG_USERS
CREATE OR REPLACE TABLE stg_users (
    id INT,
    email VARCHAR(255),
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    country VARCHAR(3),
    plan_type VARCHAR(20),
    created_at TIMESTAMP,
    last_login TIMESTAMP,
    is_active BOOLEAN,
    _loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
);

-- Table STG_PRODUCTS
CREATE OR REPLACE TABLE stg_products (
    id INT,
    merchant_id INT,
    name VARCHAR(255),
    description TEXT,
    category VARCHAR(100),
    price DECIMAL(10,2),
    stock_quantity INT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    _loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
);

-- Table STG_ORDERS
CREATE OR REPLACE TABLE stg_orders (
    id INT,
    user_id INT,
    created_at TIMESTAMP,
    total_amount DECIMAL(10,2),
    status VARCHAR(20),
    country VARCHAR(3),
    payment_method VARCHAR(50),
    _loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
);

-- Table STG_ORDER_ITEMS
CREATE OR REPLACE TABLE stg_order_items (
    id INT,
    order_id INT,
    product_id INT,
    quantity INT,
    unit_price DECIMAL(10,2),
    line_total DECIMAL(10,2),
    _loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
);

-- Table STG_CRM_CONTACTS
CREATE OR REPLACE TABLE stg_crm_contacts (
    id INT,
    email VARCHAR(255),
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    source VARCHAR(100),
    campaign_id VARCHAR(100),
    created_at TIMESTAMP,
    converted BOOLEAN,
    converted_at TIMESTAMP,
    _loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
);

-- Vérification
SHOW TABLES;

SELECT 'Tables de STAGING créées avec succès' AS message;
</code></pre>

                <ol start="2">
                    <li><div class="click-instruction">Cliquez sur <strong>"Run"</strong></div></li>
                    <li>Vous voyez les 5 tables créées dans les résultats</li>
                    <li>Les tables STAGING sont prêtes</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">9</span>Chargement des données S3 vers Snowflake (COPY INTO)</h4>
                
                <ol>
                    <li>Dans votre Worksheet, copiez-collez :</li>
                </ol>

<pre><code class="language-sql">-- ============================================
-- Chargement des données depuis S3 vers STAGING
-- ============================================

USE WAREHOUSE LOADING_WH;
USE SCHEMA STAGING;

-- 1. Chargement de STG_USERS
COPY INTO stg_users (id, email, first_name, last_name, country, plan_type, created_at, last_login, is_active)
FROM @RAW. s3_raw_stage/postgres/users/2025-11-27/  -- AJUSTEZ LA DATE SI BESOIN
FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY = '"' SKIP_HEADER = 1)
ON_ERROR = 'CONTINUE';

SELECT 'Chargement users terminé : ' || COUNT(*) || ' lignes' AS resultat FROM stg_users;

-- 2. Chargement de STG_PRODUCTS
COPY INTO stg_products (id, merchant_id, name, description, category, price, stock_quantity, created_at, updated_at)
FROM @RAW.s3_raw_stage/postgres/products/2025-11-27/
FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY = '"' SKIP_HEADER = 1)
ON_ERROR = 'CONTINUE';

SELECT 'Chargement products terminé : ' || COUNT(*) || ' lignes' AS resultat FROM stg_products;

-- 3. Chargement de STG_ORDERS
COPY INTO stg_orders (id, user_id, created_at, total_amount, status, country, payment_method)
FROM @RAW.s3_raw_stage/postgres/orders/2025-11-27/
FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY = '"' SKIP_HEADER = 1)
ON_ERROR = 'CONTINUE';

SELECT 'Chargement orders terminé : ' || COUNT(*) || ' lignes' AS resultat FROM stg_orders;

-- 4. Chargement de STG_ORDER_ITEMS
COPY INTO stg_order_items (id, order_id, product_id, quantity, unit_price, line_total)
FROM @RAW.s3_raw_stage/postgres/order_items/2025-11-27/
FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY = '"' SKIP_HEADER = 1)
ON_ERROR = 'CONTINUE';

SELECT 'Chargement order_items terminé : ' || COUNT(*) || ' lignes' AS resultat FROM stg_order_items;

-- 5. Chargement de STG_CRM_CONTACTS
COPY INTO stg_crm_contacts (id, email, first_name, last_name, source, campaign_id, created_at, converted, converted_at)
FROM @RAW.s3_raw_stage/crm/contacts/2025-11-27/
FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY = '"' SKIP_HEADER = 1)
ON_ERROR = 'CONTINUE';

SELECT 'Chargement crm_contacts terminé : ' || COUNT(*) || ' lignes' AS resultat FROM stg_crm_contacts;

-- Récapitulatif final
SELECT 'users' AS table_name, COUNT(*) AS row_count FROM stg_users
UNION ALL
SELECT 'products', COUNT(*) FROM stg_products
UNION ALL
SELECT 'orders', COUNT(*) FROM stg_orders
UNION ALL
SELECT 'order_items', COUNT(*) FROM stg_order_items
UNION ALL
SELECT 'crm_contacts', COUNT(*) FROM stg_crm_contacts;
</code></pre>

                <ol start="2">
                    <li><div class="click-instruction">Cliquez sur <strong>"Run"</strong></div></li>
                    <li>Le chargement s'exécute (durée : 10-30 secondes)</li>
                    <li>Vous voyez les messages de confirmation pour chaque table</li>
                    <li>Le récapitulatif final affiche le nombre de lignes chargées</li>
                    <li>Vos données sont maintenant dans Snowflake</li>
                </ol>
            </div>
            
            <div class="success-box">
                <h4>Félicitations</h4>
                <p>Vous avez réussi à :</p>
                <ul>
                    <li>Créer une base PostgreSQL avec des données réalistes</li>
                    <li>Configurer un Data Lake dans Amazon S3</li>
                    <li>Exporter PostgreSQL vers S3</li>
                    <li>Créer un compte Snowflake et configurer le Data Warehouse</li>
                    <li>Charger S3 vers Snowflake (COPY INTO)</li>
                </ul>
                <p><strong>Prochaine étape</strong> : Transformations avec dbt</p>
            </div>

            <!-- Section 8 - dbt -->
            <h2 id="section8">8. Installation et utilisation de dbt</h2>

            <div class="pipeline-context">
                <h3>Contexte dans le pipeline</h3>
                <p>Nous passons maintenant à la quatrième étape : dbt (data build tool).  Les données sont maintenant dans Snowflake (zone STAGING).  Avec dbt, nous allons les transformer pour créer des dimensions, des faits et des data marts optimisés pour l'analyse.</p>
                <div class="pipeline-steps">
                    <div class="pipeline-step">PostgreSQL (OLTP)</div>
                    <div class="pipeline-step">S3 (Data Lake)</div>
                    <div class="pipeline-step">Snowflake (DWH)</div>
                    <div class="pipeline-step active">dbt (Transform)</div>
                    <div class="pipeline-step">Airflow (Orchestration)</div>
                    <div class="pipeline-step">Power BI (BI)</div>
                </div>
            </div>

            <h3>8.1 Qu'est-ce que dbt ? </h3>
            <p>
                <strong>dbt</strong> (data build tool) est un framework open-source qui permet aux analystes et ingénieurs de données 
                de transformer les données dans leur Data Warehouse en écrivant simplement du SQL. 
            </p>
            
            <h4>Principes clés de dbt :</h4>
            <ul>
                <li><strong>SQL déclaratif</strong> : Vous écrivez des SELECT, dbt gère le CREATE/INSERT automatiquement</li>
                <li><strong>Modularité</strong> : Un modèle = un fichier SQL = une table/vue</li>
                <li><strong>Gestion des dépendances</strong> : dbt construit un DAG et exécute les modèles dans le bon ordre</li>
                <li><strong>Tests intégrés</strong> : Tests de données (unicité, non-nullité, relations, valeurs acceptées)</li>
                <li><strong>Documentation auto-générée</strong> : Génère un site web avec le lineage complet</li>
                <li><strong>Versionning Git</strong> : Code versionné, reviews, collaboration</li>
            </ul>

            <h3>8.2 Installation de dbt</h3>
            
            <div class="step-box">
                <h4><span class="step-number">1</span>Installation de dbt-snowflake</h4>
                
                <ol>
                    <li>Ouvrez une invite de commande (CMD ou Terminal)</li>
                    <li>Installez dbt avec l'adaptateur Snowflake :
                        <div class="command-line">
                            <code>pip install dbt-snowflake</code>
                        </div>
                    </li>
                    <li>L'installation prend 1-2 minutes</li>
                    <li>Vérifiez l'installation :
                        <div class="command-line">
                            <code>dbt --version</code>
                        </div>
                    </li>
                    <li>Vous devez voir quelque chose comme :
                        <pre><code>Core:
  - installed: 1.7.4
  - latest:    1.7.4

Plugins:
  - snowflake: 1.7.0 - Up to date! </code></pre>
                    </li>
                    <li>dbt est installé</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">2</span>Initialisation d'un projet dbt</h4>
                
                <div class="file-path">
                    <strong>Emplacement du projet dbt à créer :</strong><br>
                    Windows : <code>C:\Users\VotreNom\ShopStreamTP\dbt\</code><br>
                    macOS/Linux : <code>~/ShopStreamTP/dbt/</code>
                </div>
                
                <ol>
                    <li>Dans votre invite de commande, naviguez vers votre dossier projet :
                        <p><em>Windows :</em></p>
                        <div class="command-line">
                            <code>cd C:\Users\VotreNom\ShopStreamTP</code>
                        </div>
                        <p><em>macOS/Linux :</em></p>
                        <div class="command-line">
                            <code>cd ~/ShopStreamTP</code>
                        </div>
                    </li>
                    <li>Initialisez un nouveau projet dbt :
                        <div class="command-line">
                            <code>dbt init shopstream_dbt</code>
                        </div>
                    </li>
                    <li>dbt vous pose plusieurs questions.  Répondez comme suit :
                        <pre><code>Which database would you like to use? 
[1] snowflake

(Don't see the one you want? https://docs.getdbt.com/docs/available-adapters)

Enter a number: <strong>1</strong></code></pre>
                    </li>
                    <li>dbt crée la structure de dossiers :
                        <pre><code>shopstream_dbt/
├── dbt_project.yml
├── models/
│   └── example/
├── tests/
├── macros/
├── seeds/
└── snapshots/</code></pre>
                    </li>
                    <li>Votre projet dbt est initialisé</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">3</span>Configuration de la connexion Snowflake</h4>
                
                <div class="file-path">
                    <strong>Fichier à modifier :</strong><br>
                    Windows : <code>C:\Users\VotreNom\. dbt\profiles.yml</code><br>
                    macOS/Linux : <code>~/. dbt/profiles.yml</code>
                </div>
                
                <ol>
                    <li>Ouvrez le fichier <code>profiles.yml</code> situé dans le dossier <code>. dbt</code> de votre répertoire utilisateur</li>
                    <li>Remplacez TOUT le contenu par :</li>
                </ol>

<pre><code class="language-yaml"># Configuration dbt pour Snowflake
# Fichier : ~/. dbt/profiles.yml

shopstream_dbt:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: abc12345. eu-west-3.aws  # VOTRE account Snowflake (sans https://)
      user: VOTRE_USERNAME_SNOWFLAKE
      password: VOTRE_MOT_DE_PASSE_SNOWFLAKE
      role: ACCOUNTADMIN
      database: SHOPSTREAM_DWH
      warehouse: TRANSFORM_WH
      schema: CORE
      threads: 4
      client_session_keep_alive: False
</code></pre>

                <ol start="3">
                    <li>Modifiez les lignes suivantes avec VOS informations :
                        <ul>
                            <li><code>account</code> : Votre URL Snowflake (sans <code>https://</code> et sans <code>. snowflakecomputing.com</code>)</li>
                            <li><code>user</code> : Votre nom d'utilisateur Snowflake</li>
                            <li><code>password</code> : Votre mot de passe Snowflake</li>
                        </ul>
                    </li>
                    <li>Sauvegardez le fichier</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">4</span>Test de connexion dbt - Snowflake</h4>
                
                <ol>
                    <li>Dans votre invite de commande, naviguez vers le dossier dbt :
                        <p><em>Windows :</em></p>
                        <div class="command-line">
                            <code>cd C:\Users\VotreNom\ShopStreamTP\shopstream_dbt</code>
                        </div>
                        <p><em>macOS/Linux :</em></p>
                        <div class="command-line">
                            <code>cd ~/ShopStreamTP/shopstream_dbt</code>
                        </div>
                    </li>
                    <li>Testez la connexion :
                        <div class="command-line">
                            <code>dbt debug</code>
                        </div>
                    </li>
                    <li>Vous devez voir :
                        <pre><code>Running with dbt=1.7.4
dbt version: 1.7. 4
python version: 3.11.5
python path: ... 
os info: ...
Using profiles. yml file at ... 
Using dbt_project.yml file at ... 

Configuration:
  profiles.yml file [OK found and valid]
  dbt_project. yml file [OK found and valid]

Required dependencies:
 - git [OK found]

Connection:
  account: abc12345. eu-west-3.aws
  user: votre_user
  database: SHOPSTREAM_DWH
  warehouse: TRANSFORM_WH
  role: ACCOUNTADMIN
  schema: CORE
  Connection test: [OK connection ok]

All checks passed! </code></pre>
                    </li>
                    <li>Si vous voyez <code>All checks passed</code>, la connexion fonctionne</li>
                    <li>Si vous voyez une erreur, vérifiez vos credentials dans <code>profiles.yml</code></li>
                </ol>
            </div>

            <h3>8.3 Configuration du projet dbt</h3>
            
            <div class="step-box">
                <h4><span class="step-number">5</span>Modification du fichier dbt_project. yml</h4>
                
                <div class="file-path">
                    <strong>Fichier à modifier :</strong><br>
                    Windows : <code>C:\Users\VotreNom\ShopStreamTP\shopstream_dbt\dbt_project.yml</code><br>
                    macOS/Linux : <code>~/ShopStreamTP/shopstream_dbt/dbt_project.yml</code>
                </div>
                
                <ol>
                    <li>Ouvrez le fichier <code>dbt_project. yml</code> dans votre éditeur de texte</li>
                    <li>Remplacez TOUT le contenu par :</li>
                </ol>

<pre><code class="language-yaml"># Configuration du projet dbt ShopStream
# Fichier : dbt_project.yml

name: 'shopstream_dbt'
version: '1.0.0'
config-version: 2

# Profil à utiliser (référence profiles.yml)
profile: 'shopstream_dbt'

# Dossiers du projet
model-paths: ["models"]
analysis-paths: ["analyses"]
test-paths: ["tests"]
seed-paths: ["seeds"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]

target-path: "target"
clean-targets:
  - "target"
  - "dbt_packages"

# Configuration des modèles
models:
  shopstream_dbt:
    # Configuration par défaut
    +materialized: view
    
    # Modèles de staging
    staging:
      +materialized: view
      +schema: staging
      
    # Modèles Core (dimensions + faits)
    core:
      +materialized: table
      +schema: core
      dimensions:
        +materialized: table
      facts:
        +materialized: table
      
    # Data Marts (tables finales)
    marts:
      +materialized: table
      +schema: marts

# Variables globales
vars:
  start_date: '2022-01-01'
  premium_plans: ['premium', 'enterprise']
</code></pre>

                <ol start="3">
                    <li>Sauvegardez le fichier</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">6</span>Création de la structure de dossiers</h4>
                
                <ol>
                    <li>Dans le dossier <code>shopstream_dbt/models/</code>, supprimez le dossier <code>example</code></li>
                    <li>Créez la structure suivante :
                        <pre><code>models/
├── staging/
│   ├── _staging__sources.yml
│   ├── stg_users.sql
│   ├── stg_products.sql
│   ├── stg_orders.sql
│   └── stg_order_items. sql
├── core/
│   ├── dimensions/
│   │   ├── dim_customers.sql
│   │   ├── dim_products.sql
│   │   └── dim_date.sql
│   ├── facts/
│   │   └── fact_orders.sql
│   └── _core__schema.yml
└── marts/
    ├── mart_sales_overview.sql
    ├── mart_product_performance.sql
    └── _marts__schema.yml</code></pre>
                    </li>
                </ol>
                
                <p><strong>Pour créer cette structure :</strong></p>
                <p><em>Windows (Explorateur) :</em></p>
                <ul>
                    <li>Naviguez vers <code>C:\Users\VotreNom\ShopStreamTP\shopstream_dbt\models\</code></li>
                    <li>Supprimez le dossier <code>example</code></li>
                    <li>Créez les dossiers : <code>staging</code>, <code>core</code>, <code>marts</code></li>
                    <li>Dans <code>core</code>, créez les sous-dossiers : <code>dimensions</code>, <code>facts</code></li>
                </ul>
                
                <p><em>macOS/Linux (Terminal) :</em></p>
                <div class="command-line">
                    <code>cd ~/ShopStreamTP/shopstream_dbt/models</code><br>
                    <code>rm -rf example</code><br>
                    <code>mkdir -p staging core/dimensions core/facts marts</code>
                </div>
            </div>

            <h3>8.4 Création des modèles de staging</h3>
            
            <div class="step-box">
                <h4><span class="step-number">7</span>Déclaration des sources</h4>
                
                <div class="file-path">
                    <strong>Fichier à créer :</strong><br>
                    <code>models/staging/_staging__sources.yml</code>
                </div>
                
                <ol>
                    <li>Créez un nouveau fichier nommé <code>_staging__sources.yml</code> dans le dossier <code>models/staging/</code></li>
                    <li>Copiez-collez le contenu suivant :</li>
                </ol>

<pre><code class="language-yaml"># Déclaration des sources (tables STAGING dans Snowflake)
# Fichier : models/staging/_staging__sources.yml

version: 2

sources:
  - name: staging
    database: shopstream_dwh
    schema: staging
    description: "Tables de staging chargées depuis S3"
    
    tables:
      - name: stg_users
        description: "Utilisateurs de la plateforme"
        columns:
          - name: id
            description: "Identifiant unique de l'utilisateur"
            tests:
              - unique
              - not_null
          - name: email
            description: "Email de l'utilisateur"
            tests:
              - unique
              - not_null
              
      - name: stg_products
        description: "Catalogue de produits"
        columns:
          - name: id
            tests:
              - unique
              - not_null
              
      - name: stg_orders
        description: "Commandes"
        columns:
          - name: id
            tests:
              - unique
              - not_null
          - name: user_id
            tests:
              - not_null
              - relationships:
                  to: source('staging', 'stg_users')
                  field: id
                  
      - name: stg_order_items
        description: "Lignes de commande"
        columns:
          - name: id
            tests:
              - unique
              - not_null
          - name: order_id
            tests:
              - relationships:
                  to: source('staging', 'stg_orders')
                  field: id
</code></pre>

                <ol start="3">
                    <li>Sauvegardez le fichier</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">8</span>Modèle staging : stg_orders. sql</h4>
                
                <div class="file-path">
                    <strong>Fichier à créer :</strong><br>
                    <code>models/staging/stg_orders.sql</code>
                </div>
                
                <ol>
                    <li>Créez un nouveau fichier nommé <code>stg_orders.sql</code> dans le dossier <code>models/staging/</code></li>
                    <li>Copiez-collez le contenu suivant :</li>
                </ol>

<pre><code class="language-sql">-- Modèle de staging pour les commandes
-- Fichier : models/staging/stg_orders.sql
-- Nettoyage, typage, renommage

{{
    config(
        materialized='view',
        tags=['staging', 'orders']
    )
}}

WITH source AS (
    SELECT * FROM {{ source('staging', 'stg_orders') }}
),

renamed AS (
    SELECT
        -- Clés primaires
        id AS order_id,
        user_id AS customer_id,
        
        -- Timestamps
        created_at AS order_date,
        
        -- Métriques
        total_amount AS order_amount,
        
        -- Attributs
        UPPER(status) AS order_status,
        UPPER(country) AS country_code,
        LOWER(payment_method) AS payment_method,
        
        -- Métadonnées
        _loaded_at
        
    FROM source
    WHERE total_amount > 0
)

SELECT * FROM renamed
</code></pre>

                <ol start="3">
                    <li>Sauvegardez le fichier</li>
                </ol>
            </div>

            <h3>8.5 Création des dimensions</h3>
            
            <div class="step-box">
                <h4><span class="step-number">9</span>Dimension : dim_customers.sql</h4>
                
                <div class="file-path">
                    <strong>Fichier à créer :</strong><br>
                    <code>models/core/dimensions/dim_customers.sql</code>
                </div>
                
                <ol>
                    <li>Créez un nouveau fichier nommé <code>dim_customers.sql</code> dans le dossier <code>models/core/dimensions/</code></li>
                    <li>Copiez-collez le contenu suivant :</li>
                </ol>

<pre><code class="language-sql">-- Dimension Clients
-- Fichier : models/core/dimensions/dim_customers. sql

{{
    config(
        materialized='table',
        tags=['core', 'dimension']
    )
}}

WITH users AS (
    SELECT * FROM {{ source('staging', 'stg_users') }}
),

orders_agg AS (
    SELECT
        user_id,
        MIN(created_at) AS first_order_date,
        MAX(created_at) AS last_order_date,
        COUNT(DISTINCT id) AS total_orders,
        SUM(total_amount) AS lifetime_value
    FROM {{ source('staging', 'stg_orders') }}
    WHERE status IN ('paid', 'shipped', 'delivered')
    GROUP BY user_id
),

final AS (
    SELECT
        -- Clé primaire
        u.id AS customer_key,
        
        -- Attributs démographiques
        u.email,
        u.first_name,
        u.last_name,
        u.first_name || ' ' || u.last_name AS full_name,
        UPPER(u.country) AS country_code,
        
        -- Segmentation
        CASE
            WHEN u.plan_type IN ('premium', 'enterprise') THEN 'Premium'
            ELSE 'Freemium'
        END AS customer_segment,
        
        u.plan_type,
        u.is_active,
        
        -- Dates importantes
        u.created_at AS registration_date,
        u.last_login,
        o.first_order_date,
        o.last_order_date,
        
        -- Métriques calculées
        COALESCE(o.total_orders, 0) AS total_orders,
        COALESCE(o.lifetime_value, 0) AS lifetime_value,
        
        -- Classification RFM simplifié
        CASE
            WHEN DATEDIFF(day, o.last_order_date, CURRENT_DATE()) <= 30 THEN 'Active'
            WHEN DATEDIFF(day, o.last_order_date, CURRENT_DATE()) <= 90 THEN 'At Risk'
            WHEN o.last_order_date IS NOT NULL THEN 'Churned'
            ELSE 'No Purchase'
        END AS customer_status,
        
        -- Métadonnées
        CURRENT_TIMESTAMP() AS _dbt_updated_at
        
    FROM users u
    LEFT JOIN orders_agg o ON u.id = o.user_id
)

SELECT * FROM final
</code></pre>

                <ol start="3">
                    <li>Sauvegardez le fichier</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">10</span>Dimension : dim_products.sql</h4>
                
                <div class="file-path">
                    <strong>Fichier à créer :</strong><br>
                    <code>models/core/dimensions/dim_products.sql</code>
                </div>
                
                <ol>
                    <li>Créez un nouveau fichier nommé <code>dim_products.sql</code> dans le dossier <code>models/core/dimensions/</code></li>
                    <li>Copiez-collez le contenu suivant :</li>
                </ol>

<pre><code class="language-sql">-- Dimension Produits
-- Fichier : models/core/dimensions/dim_products. sql

{{
    config(
        materialized='table',
        tags=['core', 'dimension']
    )
}}

WITH products AS (
    SELECT * FROM {{ source('staging', 'stg_products') }}
),

product_stats AS (
    SELECT
        product_id,
        SUM(quantity) AS total_quantity_sold,
        SUM(line_total) AS total_revenue,
        COUNT(DISTINCT order_id) AS total_orders
    FROM {{ source('staging', 'stg_order_items') }}
    GROUP BY product_id
),

final AS (
    SELECT
        -- Clé primaire
        p.id AS product_key,
        
        -- Attributs
        p.name AS product_name,
        p. description AS product_description,
        p. category AS product_category,
        p.merchant_id,
        
        -- Prix
        p.price AS current_price,
        
        -- Stock
        p.stock_quantity AS current_stock,
        
        -- Métriques calculées
        COALESCE(s.total_quantity_sold, 0) AS total_quantity_sold,
        COALESCE(s.total_revenue, 0) AS total_revenue,
        COALESCE(s.total_orders, 0) AS total_orders,
        
        -- Classification
        CASE
            WHEN s.total_revenue IS NULL THEN 'No Sales'
            ELSE 'Active'
        END AS product_status,
        
        -- Dates
        p.created_at AS product_created_at,
        p.updated_at AS product_updated_at,
        
        -- Métadonnées
        CURRENT_TIMESTAMP() AS _dbt_updated_at
        
    FROM products p
    LEFT JOIN product_stats s ON p.id = s.product_id
)

SELECT * FROM final
</code></pre>

                <ol start="3">
                    <li>Sauvegardez le fichier</li>
                </ol>
            </div>

            <h3>8.6 Création de la table de faits</h3>
            
            <div class="step-box">
                <h4><span class="step-number">11</span>Fait : fact_orders.sql</h4>
                
                <div class="file-path">
                    <strong>Fichier à créer :</strong><br>
                    <code>models/core/facts/fact_orders.sql</code>
                </div>
                
                <ol>
                    <li>Créez un nouveau fichier nommé <code>fact_orders.sql</code> dans le dossier <code>models/core/facts/</code></li>
                    <li>Copiez-collez le contenu suivant :</li>
                </ol>

<pre><code class="language-sql">-- Table de faits des commandes
-- Fichier : models/core/facts/fact_orders.sql
-- Granularité : ligne de commande

{{
    config(
        materialized='table',
        tags=['core', 'fact']
    )
}}

WITH orders AS (
    SELECT * FROM {{ source('staging', 'stg_orders') }}
),

order_items AS (
    SELECT * FROM {{ source('staging', 'stg_order_items') }}
),

final AS (
    SELECT
        -- Clés de faits
        oi.id AS order_item_key,
        o.id AS order_key,
        
        -- Clés étrangères (dimensions)
        o.user_id AS customer_key,
        oi.product_id AS product_key,
        TO_DATE(o.created_at) AS date_key,
        
        -- Attributs dégénérés
        o.status AS order_status,
        o. country AS country_code,
        o.payment_method,
        
        -- Timestamps
        o.created_at AS order_timestamp,
        
        -- Métriques additives
        oi.quantity AS quantity_sold,
        oi.unit_price,
        oi.line_total AS line_revenue,
        o.total_amount AS order_total,
        
        -- Métriques dérivées
        oi.line_total * 0.2 AS estimated_margin,
        
        -- Flags
        CASE WHEN o.status IN ('paid', 'shipped', 'delivered') THEN 1 ELSE 0 END AS is_completed,
        CASE WHEN o.status = 'cancelled' THEN 1 ELSE 0 END AS is_cancelled
        
    FROM order_items oi
    INNER JOIN orders o ON oi. order_id = o.id
    WHERE o.created_at >= '{{ var("start_date") }}'
)

SELECT * FROM final
</code></pre>

                <ol start="3">
                    <li>Sauvegardez le fichier</li>
                </ol>
            </div>

            <h3>8.7 Création du data mart</h3>
            
            <div class="step-box">
                <h4><span class="step-number">12</span>Data Mart : mart_sales_overview.sql</h4>
                
                <div class="file-path">
                    <strong>Fichier à créer :</strong><br>
                    <code>models/marts/mart_sales_overview.sql</code>
                </div>
                
                <ol>
                    <li>Créez un nouveau fichier nommé <code>mart_sales_overview.sql</code> dans le dossier <code>models/marts/</code></li>
                    <li>Copiez-collez le contenu suivant :</li>
                </ol>

<pre><code class="language-sql">-- Vue d'ensemble des ventes
-- Fichier : models/marts/mart_sales_overview.sql
-- Agrégation par jour, pays, catégorie

{{
    config(
        materialized='table',
        tags=['mart', 'sales']
    )
}}

WITH fact_orders AS (
    SELECT * FROM {{ ref('fact_orders') }}
),

dim_products AS (
    SELECT * FROM {{ ref('dim_products') }}
),

dim_customers AS (
    SELECT * FROM {{ ref('dim_customers') }}
),

daily_sales AS (
    SELECT
        f.date_key AS sale_date,
        f.country_code,
        p.product_category,
        c.customer_segment,
        
        -- Métriques agrégées
        COUNT(DISTINCT f.order_key) AS total_orders,
        COUNT(DISTINCT f.customer_key) AS unique_customers,
        SUM(f.quantity_sold) AS total_quantity,
        SUM(f.line_revenue) AS total_revenue,
        SUM(f.estimated_margin) AS total_margin,
        AVG(f.order_total) AS avg_order_value,
        
        -- Ratios
        SUM(f.line_revenue) / NULLIF(COUNT(DISTINCT f.order_key), 0) AS revenue_per_order,
        SUM(f.estimated_margin) / NULLIF(SUM(f.line_revenue), 0) AS margin_rate
        
    FROM fact_orders f
    INNER JOIN dim_products p ON f.product_key = p.product_key
    INNER JOIN dim_customers c ON f.customer_key = c.customer_key
    WHERE f.is_completed = 1
    GROUP BY 1, 2, 3, 4
)

SELECT * FROM daily_sales
</code></pre>

                <ol start="3">
                    <li>Sauvegardez le fichier</li>
                </ol>
            </div>

            <h3>8.8 Exécution de dbt</h3>
            
            <div class="step-box">
                <h4><span class="step-number">13</span>Exécution des modèles dbt</h4>
                
                <ol>
                    <li>Dans votre invite de commande, assurez-vous d'être dans le dossier dbt :
                        <div class="command-line">
                            <code>cd ~/ShopStreamTP/shopstream_dbt</code>
                        </div>
                    </li>
                    <li>Exécutez tous les modèles :
                        <div class="command-line">
                            <code>dbt run</code>
                        </div>
                    </li>
                    <li>dbt compile et exécute tous les modèles dans l'ordre des dépendances</li>
                    <li>Vous voyez :
                        <pre><code>Running with dbt=1.7.4
Found 6 models, 0 tests, 0 snapshots, 0 analyses, 0 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics

Concurrency: 4 threads (target='dev')

1 of 6 START sql view model CORE.stg_orders ........................ ... [RUN]
1 of 6 OK created sql view model CORE.stg_orders . ...................... [SUCCESS in 2.34s]
2 of 6 START sql table model CORE.dim_customers ........................  [RUN]
3 of 6 START sql table model CORE.dim_products .........................  [RUN]
2 of 6 OK created sql table model CORE.dim_customers ...................  [SUCCESS in 5.67s]
3 of 6 OK created sql table model CORE.dim_products .................... [SUCCESS in 5.89s]
4 of 6 START sql table model CORE. fact_orders .......................... [RUN]
4 of 6 OK created sql table model CORE.fact_orders .....................  [SUCCESS in 8.12s]
5 of 6 START sql table model MARTS.mart_sales_overview ................ .  [RUN]
5 of 6 OK created sql table model MARTS.mart_sales_overview ............ [SUCCESS in 6.45s]

Completed successfully

Done.  PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6</code></pre>
                    </li>
                    <li>Tous les modèles sont créés dans Snowflake</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">14</span>Exécution des tests dbt</h4>
                
                <ol>
                    <li>Exécutez les tests de qualité de données :
                        <div class="command-line">
                            <code>dbt test</code>
                        </div>
                    </li>
                    <li>dbt exécute tous les tests définis dans les fichiers YAML</li>
                    <li>Vous voyez :
                        <pre><code>Running with dbt=1.7. 4
Found 6 models, 8 tests, 0 snapshots, 0 analyses, 0 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics

Concurrency: 4 threads (target='dev')

1 of 8 START test not_null_stg_users_id ................................ [RUN]
2 of 8 START test unique_stg_users_id . ................................. [RUN]
3 of 8 START test not_null_stg_users_email . ............................ [RUN]
4 of 8 START test unique_stg_users_email ... ............................ [RUN]
1 of 8 PASS not_null_stg_users_id . .... ................................. [PASS in 0.89s]
2 of 8 PASS unique_stg_users_id . ...... ................................. [PASS in 0. 92s]
3 of 8 PASS not_null_stg_users_email .. ................................. [PASS in 0. 91s]
4 of 8 PASS unique_stg_users_email ..................................... [PASS in 0.94s]
... 

Completed successfully

Done. PASS=8 WARN=0 ERROR=0 SKIP=0 TOTAL=8</code></pre>
                    </li>
                    <li>Tous les tests passent avec succès</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">15</span>Génération de la documentation</h4>
                
                <ol>
                    <li>Générez la documentation :
                        <div class="command-line">
                            <code>dbt docs generate</code>
                        </div>
                    </li>
                    <li>Lancez le serveur de documentation :
                        <div class="command-line">
                            <code>dbt docs serve</code>
                        </div>
                    </li>
                    <li>Votre navigateur s'ouvre automatiquement sur <code>http://localhost:8080</code></li>
                    <li>Vous voyez la documentation complète de votre projet dbt avec :
                        <ul>
                            <li>Le lineage graph (graphe des dépendances)</li>
                            <li>La description de chaque modèle</li>
                            <li>Les résultats des tests</li>
                            <li>Le code SQL de chaque modèle</li>
                        </ul>
                    </li>
                    <li>Fermez le serveur avec <code>Ctrl+C</code> dans le terminal</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">16</span>Vérification dans Snowflake</h4>
                
                <ol>
                    <li>Retournez dans Snowflake (interface web)</li>
                    <li>Allez dans Worksheets</li>
                    <li>Exécutez :
                        <pre><code>USE DATABASE SHOPSTREAM_DWH;
USE SCHEMA MARTS;

SELECT * FROM mart_sales_overview
LIMIT 10;</code></pre>
                    </li>
                    <li>Vous voyez les données agrégées de ventes</li>
                    <li>Vos transformations dbt sont bien dans Snowflake</li>
                </ol>
            </div>

            <div class="success-box">
                <h4>Récapitulatif dbt</h4>
                <p>Vous avez réussi à :</p>
                <ul>
                    <li>Installer dbt avec l'adaptateur Snowflake</li>
                    <li>Configurer la connexion Snowflake</li>
                    <li>Créer des modèles de staging</li>
                    <li>Créer des dimensions (dim_customers, dim_products)</li>
                    <li>Créer une table de faits (fact_orders)</li>
                    <li>Créer un data mart (mart_sales_overview)</li>
                    <li>Exécuter les transformations avec dbt run</li>
                    <li>Tester la qualité des données avec dbt test</li>
                    <li>Générer la documentation avec dbt docs</li>
                </ul>
                <p><strong>Prochaine étape</strong> : Orchestration avec Apache Airflow</p>
            </div>

            <!-- Section 9 -->
            <h2 id="section9">9.   Création des Data Marts</h2>

            <div class="pipeline-context">
                <h3>Contexte dans le pipeline</h3>
                <p>Nous restons sur l'étape dbt pour créer des data marts supplémentaires.   Ces tables sont optimisées pour répondre à des questions métier spécifiques.</p>
                <div class="pipeline-steps">
                    <div class="pipeline-step">PostgreSQL (OLTP)</div>
                    <div class="pipeline-step">S3 (Data Lake)</div>
                    <div class="pipeline-step">Snowflake (DWH)</div>
                    <div class="pipeline-step active">dbt (Transform)</div>
                    <div class="pipeline-step">Airflow (Orchestration)</div>
                    <div class="pipeline-step">Power BI (BI)</div>
                </div>
            </div>

            <h3>9.1 Data Mart : Performance Produits</h3>
            
            <div class="step-box">
                <h4><span class="step-number">1</span>Création de mart_product_performance.sql</h4>
                
                <div class="file-path">
                    <strong>Fichier à créer :</strong><br>
                    <code>models/marts/mart_product_performance. sql</code>
                </div>
                
                <ol>
                    <li>Créez un nouveau fichier nommé <code>mart_product_performance.sql</code> dans le dossier <code>models/marts/</code></li>
                    <li>Copiez-collez le contenu suivant :</li>
                </ol>

<pre><code class="language-sql">-- Performance des produits (analyse ABC)
-- Fichier : models/marts/mart_product_performance.sql

{{
    config(
        materialized='table',
        tags=['mart', 'product']
    )
}}

WITH fact_orders AS (
    SELECT * FROM {{ ref('fact_orders') }}
    WHERE is_completed = 1
),

dim_products AS (
    SELECT * FROM {{ ref('dim_products') }}
),

product_metrics AS (
    SELECT
        f.product_key,
        p.product_name,
        p.product_category,
        
        -- Volume
        COUNT(DISTINCT f.order_key) AS total_orders,
        SUM(f.quantity_sold) AS total_quantity_sold,
        
        -- Revenus
        SUM(f. line_revenue) AS total_revenue,
        AVG(f.unit_price) AS avg_unit_price,
        
        -- Marge
        SUM(f. estimated_margin) AS total_margin,
        SUM(f.estimated_margin) / NULLIF(SUM(f.line_revenue), 0) AS margin_rate,
        
        -- Dates
        MIN(f.order_timestamp) AS first_sale_date,
        MAX(f.order_timestamp) AS last_sale_date
        
    FROM fact_orders f
    INNER JOIN dim_products p ON f.product_key = p.product_key
    GROUP BY 1, 2, 3
),

ranked AS (
    SELECT
        *,
        -- Classement par revenu
        ROW_NUMBER() OVER (ORDER BY total_revenue DESC) AS revenue_rank,
        
        -- Cumul du CA
        SUM(total_revenue) OVER (ORDER BY total_revenue DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_revenue,
        
        -- CA total
        SUM(total_revenue) OVER () AS total_revenue_all
        
    FROM product_metrics
),

final AS (
    SELECT
        *,
        -- Pourcentage du CA total
        total_revenue / total_revenue_all AS revenue_pct,
        
        -- Pourcentage cumulé
        cumulative_revenue / total_revenue_all AS cumulative_revenue_pct,
        
        -- Classification ABC
        CASE
            WHEN cumulative_revenue / total_revenue_all <= 0.8 THEN 'A'
            WHEN cumulative_revenue / total_revenue_all <= 0.95 THEN 'B'
            ELSE 'C'
        END AS abc_class,
        
        -- Performance
        CASE
            WHEN revenue_rank <= 10 THEN 'Top 10'
            WHEN revenue_rank <= 50 THEN 'Top 50'
            ELSE 'Long Tail'
        END AS performance_tier
        
    FROM ranked
)

SELECT
    product_key,
    product_name,
    product_category,
    total_orders,
    total_quantity_sold,
    total_revenue,
    avg_unit_price,
    total_margin,
    margin_rate,
    first_sale_date,
    last_sale_date,
    revenue_rank,
    revenue_pct,
    cumulative_revenue_pct,
    abc_class,
    performance_tier
FROM final
ORDER BY revenue_rank
</code></pre>

                <ol start="3">
                    <li>Sauvegardez le fichier</li>
                    <li>Exécutez dbt pour créer ce nouveau mart :
                        <div class="command-line">
                            <code>dbt run --select mart_product_performance</code>
                        </div>
                    </li>
                    <li>Le mart est créé dans Snowflake</li>
                </ol>
            </div>

            <h3>9.2 Data Mart : Customer Lifetime Value</h3>
            
            <div class="step-box">
                <h4><span class="step-number">2</span>Création de mart_customer_ltv.sql</h4>
                
                <div class="file-path">
                    <strong>Fichier à créer :</strong><br>
                    <code>models/marts/mart_customer_ltv.sql</code>
                </div>
                
                <ol>
                    <li>Créez un nouveau fichier nommé <code>mart_customer_ltv. sql</code> dans le dossier <code>models/marts/</code></li>
                    <li>Copiez-collez le contenu suivant :</li>
                </ol>

<pre><code class="language-sql">-- Customer Lifetime Value et segmentation RFM
-- Fichier : models/marts/mart_customer_ltv.sql

{{
    config(
        materialized='table',
        tags=['mart', 'customer']
    )
}}

WITH dim_customers AS (
    SELECT * FROM {{ ref('dim_customers') }}
),

fact_orders AS (
    SELECT * FROM {{ ref('fact_orders') }}
    WHERE is_completed = 1
),

customer_metrics AS (
    SELECT
        f.customer_key,
        
        -- Récence (Recency)
        MAX(f.order_timestamp) AS last_order_date,
        DATEDIFF(day, MAX(f.order_timestamp), CURRENT_DATE()) AS days_since_last_order,
        
        -- Fréquence (Frequency)
        COUNT(DISTINCT f.order_key) AS total_orders,
        DATEDIFF(day, MIN(f.order_timestamp), MAX(f.order_timestamp)) AS customer_lifespan_days,
        
        -- Montant (Monetary)
        SUM(f.line_revenue) AS lifetime_value,
        AVG(f.order_total) AS avg_order_value,
        
        -- Dates
        MIN(f.order_timestamp) AS first_order_date
        
    FROM fact_orders f
    GROUP BY 1
),

rfm_scores AS (
    SELECT
        *,
        -- Score RFM (1-5, 5 étant le meilleur)
        NTILE(5) OVER (ORDER BY days_since_last_order ASC) AS recency_score,
        NTILE(5) OVER (ORDER BY total_orders DESC) AS frequency_score,
        NTILE(5) OVER (ORDER BY lifetime_value DESC) AS monetary_score
        
    FROM customer_metrics
),

rfm_segments AS (
    SELECT
        *,
        recency_score + frequency_score + monetary_score AS rfm_total_score,
        
        -- Segmentation RFM
        CASE
            WHEN recency_score >= 4 AND frequency_score >= 4 AND monetary_score >= 4 THEN 'Champions'
            WHEN recency_score >= 3 AND frequency_score >= 3 THEN 'Loyal Customers'
            WHEN recency_score >= 4 AND frequency_score <= 2 THEN 'Promising'
            WHEN recency_score >= 3 AND monetary_score >= 3 THEN 'Potential Loyalists'
            WHEN recency_score <= 2 AND frequency_score >= 3 THEN 'At Risk'
            WHEN recency_score <= 2 AND monetary_score >= 4 THEN 'Cant Lose Them'
            WHEN recency_score <= 2 THEN 'Hibernating'
            ELSE 'Others'
        END AS rfm_segment
        
    FROM rfm_scores
),

final AS (
    SELECT
        c.customer_key,
        c.email,
        c.full_name,
        c.country_code,
        c.customer_segment,
        c.customer_status,
        
        -- Métriques RFM
        r.last_order_date,
        r.days_since_last_order,
        r.total_orders,
        r.customer_lifespan_days,
        r.lifetime_value,
        r.avg_order_value,
        r.first_order_date,
        
        -- Scores et segments
        r.recency_score,
        r.frequency_score,
        r.monetary_score,
        r.rfm_total_score,
        r.rfm_segment,
        
        -- Risque de churn
        CASE
            WHEN r.days_since_last_order > 180 THEN 'High'
            WHEN r.days_since_last_order > 90 THEN 'Medium'
            ELSE 'Low'
        END AS churn_risk
        
    FROM dim_customers c
    INNER JOIN rfm_segments r ON c.customer_key = r. customer_key
)

SELECT * FROM final
</code></pre>

                <ol start="3">
                    <li>Sauvegardez le fichier</li>
                    <li>Exécutez dbt :
                        <div class="command-line">
                            <code>dbt run --select mart_customer_ltv</code>
                        </div>
                    </li>
                    <li>Le mart est créé dans Snowflake</li>
                </ol>
            </div>

            <div class="success-box">
                <h4>Data Marts créés</h4>
                <p>Vous disposez maintenant de 3 data marts dans Snowflake :</p>
                <ul>
                    <li><strong>mart_sales_overview</strong> : Vue d'ensemble des ventes par jour, pays, catégorie</li>
                    <li><strong>mart_product_performance</strong> : Performance des produits avec analyse ABC</li>
                    <li><strong>mart_customer_ltv</strong> : Lifetime Value et segmentation RFM des clients</li>
                </ul>
                <p>Ces tables sont prêtes à être consommées par Power BI. </p>
            </div>

            <!-- Section 10 -->
            <h2 id="section10">10.  Installation et configuration d'Apache Airflow</h2>

            <div class="pipeline-context">
                <h3>Contexte dans le pipeline</h3>
                <p>Nous passons maintenant à la cinquième étape : Apache Airflow.   Jusqu'ici, nous avons exécuté chaque étape manuellement.  Airflow va orchestrer automatiquement l'ensemble du pipeline : extraction PostgreSQL vers S3, chargement S3 vers Snowflake, transformations dbt, et rafraîchissement BI.</p>
                <div class="pipeline-steps">
                    <div class="pipeline-step">PostgreSQL (OLTP)</div>
                    <div class="pipeline-step">S3 (Data Lake)</div>
                    <div class="pipeline-step">Snowflake (DWH)</div>
                    <div class="pipeline-step">dbt (Transform)</div>
                    <div class="pipeline-step active">Airflow (Orchestration)</div>
                    <div class="pipeline-step">Power BI (BI)</div>
                </div>
            </div>

            <!-- REPRISE SECTION 10 COMPLETE -->
            <h2 id="section10">10.  Installation et configuration d'Apache Airflow</h2>

            <div class="pipeline-context">
                <h3>Contexte dans le pipeline</h3>
                <p>Nous passons maintenant à la cinquième étape : Apache Airflow.  Jusqu'ici, nous avons exécuté chaque étape manuellement. Airflow va orchestrer automatiquement l'ensemble du pipeline : extraction PostgreSQL vers S3, chargement S3 vers Snowflake, transformations dbt, et rafraîchissement BI.</p>
                <div class="pipeline-steps">
                    <div class="pipeline-step">PostgreSQL (OLTP)</div>
                    <div class="pipeline-step">S3 (Data Lake)</div>
                    <div class="pipeline-step">Snowflake (DWH)</div>
                    <div class="pipeline-step">dbt (Transform)</div>
                    <div class="pipeline-step active">Airflow (Orchestration)</div>
                    <div class="pipeline-step">Power BI (BI)</div>
                </div>
            </div>

            <h3>10.1 Qu'est-ce qu'Apache Airflow ?</h3>
            <p>
                <strong>Apache Airflow</strong> est une plateforme open-source d'orchestration de workflows.  
                Elle permet de définir, planifier et monitorer des pipelines de données complexes sous forme de 
                <strong>DAGs</strong> (Directed Acyclic Graphs). 
            </p>
            
            <h4>Concepts clés d'Airflow :</h4>
            <ul>
                <li><strong>DAG</strong> : Un graphe orienté acyclique qui définit un workflow</li>
                <li><strong>Task</strong> : Une unité de travail (BashOperator, PythonOperator, SnowflakeOperator)</li>
                <li><strong>Operator</strong> : Un template de tâche réutilisable</li>
                <li><strong>Sensor</strong> : Attend qu'une condition soit remplie (ex : fichier présent dans S3)</li>
                <li><strong>Schedule</strong> : Fréquence d'exécution (expression cron)</li>
                <li><strong>Executor</strong> : Mécanisme d'exécution (Local, Celery, Kubernetes)</li>
            </ul>

            <h3>10.2 Installation d'Airflow</h3>
            
            <div class="step-box">
                <h4><span class="step-number">1</span>Installation d'Apache Airflow</h4>
                
                <ol>
                    <li>Ouvrez une invite de commande (CMD ou Terminal)</li>
                    <li>Définissez la version d'Airflow à installer :
                        <div class="command-line">
                            <code>export AIRFLOW_VERSION=2. 8.0</code><br>
                            <code>export PYTHON_VERSION="$(python --version | cut -d " " -f 2 | cut -d "." -f 1-2)"</code><br>
                            <code>export CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"</code>
                        </div>
                        <p><em>Sous Windows (CMD), utilisez plutôt :</em></p>
                        <div class="command-line">
                            <code>set AIRFLOW_VERSION=2.8.0</code><br>
                            <code>set CONSTRAINT_URL=https://raw.githubusercontent.com/apache/airflow/constraints-2.8.0/constraints-3.11.txt</code>
                        </div>
                    </li>
                    <li>Installez Airflow avec les providers nécessaires :
                        <div class="command-line">
                            <code>pip install "apache-airflow[postgres,amazon,snowflake]==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"</code>
                        </div>
                        <p><em>Sous Windows :</em></p>
                        <div class="command-line">
                            <code>pip install "apache-airflow[postgres,amazon,snowflake]==2.8.0" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.8.0/constraints-3.11.txt"</code>
                        </div>
                    </li>
                    <li>L'installation prend 5-10 minutes</li>
                    <li>Vérifiez l'installation :
                        <div class="command-line">
                            <code>airflow version</code>
                        </div>
                    </li>
                    <li>Vous devez voir : <code>2.8.0</code></li>
                    <li>Airflow est installé</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">2</span>Configuration du dossier Airflow</h4>
                
                <div class="file-path">
                    <strong>Dossier Airflow à créer :</strong><br>
                    Windows : <code>C:\Users\VotreNom\ShopStreamTP\airflow\</code><br>
                    macOS/Linux : <code>~/ShopStreamTP/airflow/</code>
                </div>
                
                <ol>
                    <li>Définissez la variable d'environnement AIRFLOW_HOME :
                        <p><em>Windows (CMD) :</em></p>
                        <div class="command-line">
                            <code>set AIRFLOW_HOME=C:\Users\VotreNom\ShopStreamTP\airflow</code>
                        </div>
                        <p><em>macOS/Linux :</em></p>
                        <div class="command-line">
                            <code>export AIRFLOW_HOME=~/ShopStreamTP/airflow</code>
                        </div>
                    </li>
                    <li>Initialisez la base de données Airflow :
                        <div class="command-line">
                            <code>airflow db init</code>
                        </div>
                    </li>
                    <li>La commande crée la structure de dossiers :
                        <pre><code>airflow/
├── airflow. cfg
├── airflow.db
├── dags/
├── logs/
└── plugins/</code></pre>
                    </li>
                    <li>Le dossier Airflow est initialisé</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">3</span>Création d'un utilisateur admin</h4>
                
                <ol>
                    <li>Créez un utilisateur administrateur :
                        <div class="command-line">
                            <code>airflow users create \</code><br>
                            <code>    --username admin \</code><br>
                            <code>    --firstname Admin \</code><br>
                            <code>    --lastname User \</code><br>
                            <code>    --role Admin \</code><br>
                            <code>    --email admin@shopstream.com \</code><br>
                            <code>    --password admin123</code>
                        </div>
                        <p><em>Sous Windows (tout sur une ligne) :</em></p>
                        <div class="command-line">
                            <code>airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@shopstream.com --password admin123</code>
                        </div>
                    </li>
                    <li>Vous voyez : <code>User "admin" created</code></li>
                    <li>L'utilisateur admin est créé</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">4</span>Démarrage d'Airflow</h4>
                
                <ol>
                    <li>Ouvrez <strong>deux</strong> invites de commande (ou deux onglets Terminal)</li>
                    <li><strong>Terminal 1</strong> : Démarrez le scheduler :
                        <div class="command-line">
                            <code>airflow scheduler</code>
                        </div>
                    </li>
                    <li>Le scheduler démarre.  Laissez ce terminal ouvert</li>
                    <li><strong>Terminal 2</strong> : Démarrez le webserver :
                        <div class="command-line">
                            <code>airflow webserver --port 8080</code>
                        </div>
                    </li>
                    <li>Le webserver démarre (peut prendre 30 secondes)</li>
                    <li>Vous voyez : <code>Listening at: http://0.0.0.0:8080</code></li>
                    <li>Ouvrez votre navigateur et allez sur : <code>http://localhost:8080</code></li>
                    <li>La page de connexion Airflow s'affiche</li>
                    <li>Connectez-vous avec :
                        <ul>
                            <li>Username : <code>admin</code></li>
                            <li>Password : <code>admin123</code></li>
                        </ul>
                    </li>
                    <li>Vous voyez le tableau de bord Airflow</li>
                    <li>Airflow est opérationnel</li>
                </ol>
            </div>

            <h3>10.3 Configuration des connexions Airflow</h3>
            
            <div class="step-box">
                <h4><span class="step-number">5</span>Configuration de la connexion AWS</h4>
                
                <ol>
                    <li>Dans l'interface Airflow (http://localhost:8080)</li>
                    <li>Allez dans le menu : <strong>Admin > Connections</strong></li>
                    <li><div class="click-instruction">Cliquez sur le bouton <strong>"+"</strong> (Add a new record)</div></li>
                    <li>Remplissez le formulaire :
                        <ul>
                            <li><strong>Connection Id</strong> : <code>aws_default</code></li>
                            <li><strong>Connection Type</strong> : <code>Amazon Web Services</code></li>
                            <li><strong>AWS Access Key ID</strong> : Votre Access Key ID</li>
                            <li><strong>AWS Secret Access Key</strong> : Votre Secret Access Key</li>
                            <li><strong>Extra</strong> : <code>{"region_name": "eu-west-3"}</code></li>
                        </ul>
                    </li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Save"</strong></div></li>
                    <li>La connexion AWS est configurée</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">6</span>Configuration de la connexion Snowflake</h4>
                
                <ol>
                    <li>Dans Airflow, allez dans <strong>Admin > Connections</strong></li>
                    <li><div class="click-instruction">Cliquez sur le bouton <strong>"+"</strong></div></li>
                    <li>Remplissez le formulaire :
                        <ul>
                            <li><strong>Connection Id</strong> : <code>snowflake_default</code></li>
                            <li><strong>Connection Type</strong> : <code>Snowflake</code></li>
                            <li><strong>Account</strong> : Votre account Snowflake (ex : <code>abc12345.eu-west-3.aws</code>)</li>
                            <li><strong>User</strong> : Votre username Snowflake</li>
                            <li><strong>Password</strong> : Votre mot de passe Snowflake</li>
                            <li><strong>Database</strong> : <code>SHOPSTREAM_DWH</code></li>
                            <li><strong>Schema</strong> : <code>STAGING</code></li>
                            <li><strong>Warehouse</strong> : <code>LOADING_WH</code></li>
                            <li><strong>Role</strong> : <code>ACCOUNTADMIN</code></li>
                        </ul>
                    </li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Save"</strong></div></li>
                    <li>La connexion Snowflake est configurée</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">7</span>Configuration de la connexion PostgreSQL</h4>
                
                <ol>
                    <li>Dans Airflow, allez dans <strong>Admin > Connections</strong></li>
                    <li><div class="click-instruction">Cliquez sur le bouton <strong>"+"</strong></div></li>
                    <li>Remplissez le formulaire :
                        <ul>
                            <li><strong>Connection Id</strong> : <code>postgres_shopstream</code></li>
                            <li><strong>Connection Type</strong> : <code>Postgres</code></li>
                            <li><strong>Host</strong> : <code>localhost</code></li>
                            <li><strong>Database</strong> : <code>shopstream</code></li>
                            <li><strong>Login</strong> : <code>postgres</code></li>
                            <li><strong>Password</strong> : Votre mot de passe PostgreSQL</li>
                            <li><strong>Port</strong> : <code>5432</code></li>
                        </ul>
                    </li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Save"</strong></div></li>
                    <li>La connexion PostgreSQL est configurée</li>
                </ol>
            </div>

            <h3>10.4 Création du DAG Airflow</h3>
            
            <div class="step-box">
                <h4><span class="step-number">8</span>Création du fichier DAG</h4>
                
                <div class="file-path">
                    <strong>Fichier à créer :</strong><br>
                    Windows : <code>C:\Users\VotreNom\ShopStreamTP\airflow\dags\shopstream_daily_pipeline.py</code><br>
                    macOS/Linux : <code>~/ShopStreamTP/airflow/dags/shopstream_daily_pipeline.py</code>
                </div>
                
                <ol>
                    <li>Créez un nouveau fichier nommé <code>shopstream_daily_pipeline.py</code> dans le dossier <code>airflow/dags/</code></li>
                    <li>Copiez-collez le contenu suivant :</li>
                </ol>

<pre><code class="language-python">"""
shopstream_daily_pipeline.py
DAG Airflow pour le pipeline quotidien ShopStream

Ce DAG orchestre :
1. Extraction PostgreSQL vers S3
2. Chargement S3 vers Snowflake Staging
3. Transformations dbt (staging vers core vers marts)
4. Tests de qualité dbt

Emplacement : airflow/dags/shopstream_daily_pipeline.py
Auteur : Data Engineering Team
Date : 2025-11-27
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators. python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator

# Configuration du DAG
default_args = {
    'owner': 'data_engineering',
    'depends_on_past': False,
    'start_date': datetime(2025, 11, 1),
    'email': ['alerts@shopstream.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2)
}

dag = DAG(
    'shopstream_daily_pipeline',
    default_args=default_args,
    description='Pipeline quotidien ShopStream : PostgreSQL vers S3 vers Snowflake vers dbt vers BI',
    schedule_interval='0 2 * * *',  # Tous les jours à 2h du matin
    catchup=False,
    tags=['production', 'daily', 'shopstream']
)

# Tâches Python
def extract_postgres_to_s3(**context):
    """Extrait les données PostgreSQL et les pousse dans S3"""
    import subprocess
    execution_date = context['ds']  # Date d'exécution (YYYY-MM-DD)
    
    print(f"Extraction PostgreSQL vers S3 pour {execution_date}")
    
    # Exécution du script Python d'export
    # MODIFIEZ LE CHEMIN SELON VOTRE INSTALLATION
    result = subprocess.run(
        ['python', '/chemin/vers/ShopStreamTP/scripts/export_to_s3.py'],
        capture_output=True,
        text=True
    )
    
    if result.returncode != 0:
        raise Exception(f"Erreur lors de l'export : {result.stderr}")
    
    print(result.stdout)
    print("Export terminé avec succès")

def run_dbt_models(**context):
    """Exécute les transformations dbt"""
    import subprocess
    
    print("Exécution des modèles dbt")
    
    # MODIFIEZ LE CHEMIN SELON VOTRE INSTALLATION
    result = subprocess.run(
        ['dbt', 'run', '--project-dir', '/chemin/vers/ShopStreamTP/shopstream_dbt'],
        capture_output=True,
        text=True
    )
    
    if result.returncode != 0:
        raise Exception(f"Erreur dbt run : {result.stderr}")
    
    print(result.stdout)
    print("Transformations dbt terminées")

def run_dbt_tests(**context):
    """Exécute les tests de qualité dbt"""
    import subprocess
    
    print("Exécution des tests dbt")
    
    # MODIFIEZ LE CHEMIN SELON VOTRE INSTALLATION
    result = subprocess.run(
        ['dbt', 'test', '--project-dir', '/chemin/vers/ShopStreamTP/shopstream_dbt'],
        capture_output=True,
        text=True
    )
    
    if result.returncode != 0:
        print(f"Certains tests ont échoué : {result.stderr}")
        # On ne fait pas échouer le DAG, juste un warning
    
    print(result.stdout)
    print("Tests dbt terminés")

def send_success_notification(**context):
    """Envoie une notification de succès"""
    print("Pipeline terminé avec succès")
    print(f"Execution date : {context['ds']}")
    # Ici : appel à Slack/Teams/Email via webhook

# Définition des tâches

# Tâche 1 : Extraction PostgreSQL vers S3
task_extract = PythonOperator(
    task_id='extract_postgres_to_s3',
    python_callable=extract_postgres_to_s3,
    dag=dag
)

# Tâche 2 : Attendre que les fichiers soient présents dans S3
task_wait_s3_users = S3KeySensor(
    task_id='wait_s3_users_file',
    bucket_name='shopstream-datalake-votreprenom',  # MODIFIEZ avec votre bucket
    bucket_key='raw/postgres/users/{{ ds }}/users_{{ ds_nodash }}.csv',
    aws_conn_id='aws_default',
    timeout=600,
    poke_interval=30,
    dag=dag
)

task_wait_s3_orders = S3KeySensor(
    task_id='wait_s3_orders_file',
    bucket_name='shopstream-datalake-votreprenom',  # MODIFIEZ avec votre bucket
    bucket_key='raw/postgres/orders/{{ ds }}/orders_{{ ds_nodash }}.csv',
    aws_conn_id='aws_default',
    timeout=600,
    poke_interval=30,
    dag=dag
)

# Tâche 3 : Chargement S3 vers Snowflake STAGING
task_load_staging_users = SnowflakeOperator(
    task_id='load_staging_users',
    snowflake_conn_id='snowflake_default',
    sql="""
        USE WAREHOUSE LOADING_WH;
        USE SCHEMA SHOPSTREAM_DWH. STAGING;
        
        TRUNCATE TABLE stg_users;
        
        COPY INTO stg_users (id, email, first_name, last_name, country, plan_type, created_at, last_login, is_active)
        FROM @RAW. s3_raw_stage/postgres/users/{{ ds }}/
        FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY = '"' SKIP_HEADER = 1)
        ON_ERROR = 'CONTINUE';
    """,
    dag=dag
)

task_load_staging_orders = SnowflakeOperator(
    task_id='load_staging_orders',
    snowflake_conn_id='snowflake_default',
    sql="""
        USE WAREHOUSE LOADING_WH;
        USE SCHEMA SHOPSTREAM_DWH. STAGING;
        
        TRUNCATE TABLE stg_orders;
        
        COPY INTO stg_orders (id, user_id, created_at, total_amount, status, country, payment_method)
        FROM @RAW.s3_raw_stage/postgres/orders/{{ ds }}/
        FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY = '"' SKIP_HEADER = 1)
        ON_ERROR = 'CONTINUE';
    """,
    dag=dag
)

# Tâche 4 : Exécution de dbt (staging vers core vers marts)
task_dbt_run = PythonOperator(
    task_id='dbt_run_models',
    python_callable=run_dbt_models,
    dag=dag
)

# Tâche 5 : Tests dbt
task_dbt_test = PythonOperator(
    task_id='dbt_test_models',
    python_callable=run_dbt_tests,
    dag=dag
)

# Tâche 6 : Génération de la documentation dbt
task_dbt_docs = BashOperator(
    task_id='dbt_generate_docs',
    bash_command='cd /chemin/vers/ShopStreamTP/shopstream_dbt && dbt docs generate',  # MODIFIEZ le chemin
    dag=dag
)

# Tâche 7 : Notification de succès
task_success = PythonOperator(
    task_id='send_success_notification',
    python_callable=send_success_notification,
    dag=dag
)

# Définition des dépendances (DAG)

# Phase 1 : Extraction
task_extract >> [task_wait_s3_users, task_wait_s3_orders]

# Phase 2 : Chargement Staging
task_wait_s3_users >> task_load_staging_users
task_wait_s3_orders >> task_load_staging_orders

# Phase 3 : Transformations dbt
[task_load_staging_users, task_load_staging_orders] >> task_dbt_run

# Phase 4 : Tests et documentation
task_dbt_run >> task_dbt_test
task_dbt_test >> task_dbt_docs

# Phase 5 : Notification
task_dbt_docs >> task_success
</code></pre>

                <ol start="3">
                    <li>Sauvegardez le fichier</li>
                    <li><strong>IMPORTANT</strong> : Modifiez les lignes suivantes :
                        <ul>
                            <li>Ligne 67 et 78 : Chemin vers <code>export_to_s3.py</code> et projet dbt</li>
                            <li>Ligne 109 et 121 : Nom de votre bucket S3</li>
                            <li>Ligne 190 : Chemin vers le projet dbt</li>
                        </ul>
                    </li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">9</span>Activation et exécution du DAG</h4>
                
                <ol>
                    <li>Retournez dans l'interface Airflow (http://localhost:8080)</li>
                    <li>Rafraîchissez la page</li>
                    <li>Vous devez voir le DAG <strong>shopstream_daily_pipeline</strong> dans la liste</li>
                    <li>Par défaut, le DAG est en pause (toggle désactivé)</li>
                    <li><div class="click-instruction">Cliquez sur le toggle à gauche du nom du DAG pour l'activer</div></li>
                    <li>Le DAG est maintenant actif (toggle bleu)</li>
                    <li>Pour tester le DAG manuellement :
                        <ul>
                            <li><div class="click-instruction">Cliquez sur le nom du DAG <strong>shopstream_daily_pipeline</strong></div></li>
                            <li>Vous voyez la vue détaillée du DAG avec le graphe</li>
                            <li><div class="click-instruction">Cliquez sur le bouton <strong>"Play"</strong> (triangle) en haut à droite</div></li>
                            <li><div class="click-instruction">Sélectionnez <strong>"Trigger DAG"</strong></div></li>
                        </ul>
                    </li>
                    <li>Le DAG démarre</li>
                    <li>Vous voyez les tâches s'exécuter une par une :
                        <ul>
                            <li>Vert : Succès</li>
                            <li>Jaune : En cours</li>
                            <li>Rouge : Échec</li>
                        </ul>
                    </li>
                    <li>Cliquez sur une tâche pour voir les logs détaillés</li>
                    <li>Le pipeline s'exécute automatiquement</li>
                </ol>
            </div>

            <h3>10.5 Visualisation du DAG</h3>
            
            <div class="mermaid">
graph LR
    A[extract_postgres_to_s3] --> B[wait_s3_users_file]
    A --> C[wait_s3_orders_file]
    
    B --> D[load_staging_users]
    C --> E[load_staging_orders]
    
    D --> F[dbt_run_models]
    E --> F
    
    F --> G[dbt_test_models]
    G --> H[dbt_generate_docs]
    H --> I[send_success_notification]
    
    style A fill:#e3f2fd
    style B fill:#fff9c4
    style C fill:#fff9c4
    style D fill:#c8e6c9
    style E fill:#c8e6c9
    style F fill:#e1bee7
    style G fill:#ffccbc
    style H fill:#d1c4e9
    style I fill:#c5e1a5
            </div>
            <p class="diagram-caption">Figure 3 : DAG Airflow du pipeline quotidien</p>

            <div class="success-box">
                <h4>Récapitulatif Airflow</h4>
                <p>Vous avez réussi à :</p>
                <ul>
                    <li>Installer Apache Airflow avec les providers nécessaires</li>
                    <li>Configurer les connexions (AWS, Snowflake, PostgreSQL)</li>
                    <li>Créer un DAG complet orchestrant tout le pipeline</li>
                    <li>Activer et exécuter le DAG</li>
                    <li>Surveiller l'exécution via l'interface web</li>
                </ul>
                <p><strong>Prochaine étape</strong> : Visualisation avec Power BI</p>
            </div>

            <!-- Section 11 -->
            <h2 id="section11">11. Connexion Power BI et création de dashboards</h2>

            <div class="pipeline-context">
                <h3>Contexte dans le pipeline</h3>
                <p>Nous arrivons à la dernière étape technique : Power BI.  Les données sont maintenant transformées et disponibles dans les data marts de Snowflake.  Nous allons les connecter à Power BI pour créer des dashboards interactifs destinés aux utilisateurs métier.</p>
                <div class="pipeline-steps">
                    <div class="pipeline-step">PostgreSQL (OLTP)</div>
                    <div class="pipeline-step">S3 (Data Lake)</div>
                    <div class="pipeline-step">Snowflake (DWH)</div>
                    <div class="pipeline-step">dbt (Transform)</div>
                    <div class="pipeline-step">Airflow (Orchestration)</div>
                    <div class="pipeline-step active">Power BI (BI)</div>
                </div>
            </div>

            <h3>11.1 Pourquoi Power BI ?</h3>
            <p>
                <strong>Power BI</strong> est l'outil de BI de Microsoft, largement adopté en entreprise. 
                Il offre une intégration native avec l'écosystème Microsoft et une courbe d'apprentissage douce 
                pour les utilisateurs métier.
            </p>
            
            <h4>Alternatives :</h4>
            <table>
                <tr>
                    <th>Outil</th>
                    <th>Avantages</th>
                    <th>Inconvénients</th>
                </tr>
                <tr>
                    <td><strong>Power BI</strong></td>
                    <td>Intégration Microsoft, prix attractif, DAX puissant</td>
                    <td>Moins flexible que Tableau</td>
                </tr>
                <tr>
                    <td><strong>Tableau</strong></td>
                    <td>Visualisations riches, communauté active</td>
                    <td>Coût élevé</td>
                </tr>
                <tr>
                    <td><strong>Looker</strong></td>
                    <td>LookML (code-first), excellent lineage</td>
                    <td>Nécessite des compétences techniques</td>
                </tr>
                <tr>
                    <td><strong>Metabase</strong></td>
                    <td>Open-source, simple, gratuit</td>
                    <td>Moins de fonctionnalités avancées</td>
                </tr>
            </table>

            <h3>11.2 Installation de Power BI Desktop</h3>
            
            <div class="step-box">
                <h4><span class="step-number">1</span>Téléchargement de Power BI Desktop</h4>
                
                <ol>
                    <li>Allez sur <a href="https://powerbi. microsoft.com/fr-fr/downloads/" target="_blank">https://powerbi.microsoft. com/fr-fr/downloads/</a></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Télécharger gratuitement"</strong> sous "Power BI Desktop"</div></li>
                    <li>Le fichier <code>PBIDesktopSetup_x64.exe</code> se télécharge (environ 500 Mo)</li>
                    <li>Double-cliquez sur le fichier téléchargé</li>
                    <li>L'assistant d'installation s'ouvre</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Next"</strong></div></li>
                    <li>Acceptez les conditions d'utilisation</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Next"</strong></div></li>
                    <li>Laissez le dossier d'installation par défaut</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Install"</strong></div></li>
                    <li>L'installation prend 2-3 minutes</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Finish"</strong></div></li>
                    <li>Power BI Desktop s'ouvre automatiquement</li>
                </ol>
                
                <div class="info-box">
                    <strong>Note</strong> : Power BI Desktop est disponible uniquement pour Windows. 
                    Les utilisateurs Mac peuvent utiliser Power BI Service (version web) ou une machine virtuelle Windows.
                </div>
            </div>

            <h3>11.3 Connexion à Snowflake</h3>
            
            <div class="step-box">
                <h4><span class="step-number">2</span>Ajout de la source de données Snowflake</h4>
                
                <ol>
                    <li>Dans Power BI Desktop, fermez la fenêtre de démarrage si elle apparaît</li>
                    <li>Dans le ruban supérieur, allez dans l'onglet <strong>"Accueil"</strong></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Obtenir des données"</strong></div></li>
                    <li>Dans la fenêtre qui s'ouvre, tapez <code>Snowflake</code> dans la barre de recherche</li>
                    <li><div class="click-instruction">Sélectionnez <strong>"Snowflake"</strong></div></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Connecter"</strong></div></li>
                    <li>Une fenêtre "Snowflake" s'ouvre</li>
                    <li>Remplissez les champs :
                        <ul>
                            <li><strong>Serveur</strong> : Votre URL Snowflake (ex : <code>abc12345.eu-west-3.aws.snowflakecomputing.com</code>)</li>
                            <li><strong>Warehouse</strong> : <code>BI_WH</code></li>
                        </ul>
                    </li>
                    <li>Laissez les autres options par défaut</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"OK"</strong></div></li>
                    <li>Une fenêtre d'authentification apparaît</li>
                    <li>Sélectionnez l'onglet <strong>"Base de données"</strong></li>
                    <li>Entrez :
                        <ul>
                            <li><strong>Nom d'utilisateur</strong> : Votre username Snowflake</li>
                            <li><strong>Mot de passe</strong> : Votre mot de passe Snowflake</li>
                        </ul>
                    </li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Connecter"</strong></div></li>
                    <li>Power BI se connecte à Snowflake (peut prendre 10-20 secondes)</li>
                    <li>La fenêtre "Navigateur" s'ouvre avec la liste des bases de données</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">3</span>Sélection des tables Data Marts</h4>
                
                <ol>
                    <li>Dans la fenêtre "Navigateur", développez l'arborescence :
                        <ul>
                            <li><div class="click-instruction">Cliquez sur le triangle à côté de <strong>"SHOPSTREAM_DWH"</strong></div></li>
                            <li><div class="click-instruction">Cliquez sur le triangle à côté de <strong>"MARTS"</strong></div></li>
                        </ul>
                    </li>
                    <li>Vous voyez les 3 tables de data marts :
                        <ul>
                            <li><code>MART_SALES_OVERVIEW</code></li>
                            <li><code>MART_PRODUCT_PERFORMANCE</code></li>
                            <li><code>MART_CUSTOMER_LTV</code></li>
                        </ul>
                    </li>
                    <li>Cochez les 3 tables</li>
                    <li>En bas de la fenêtre, vous avez deux options :
                        <ul>
                            <li><strong>Charger</strong> : Importe les données dans Power BI (mode Import)</li>
                            <li><strong>Transformer les données</strong> : Ouvre Power Query Editor</li>
                        </ul>
                    </li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Charger"</strong></div></li>
                    <li>Power BI charge les données (durée : 30 secondes à 2 minutes selon le volume)</li>
                    <li>Vous voyez les 3 tables apparaître dans le panneau "Champs" à droite</li>
                    <li>Les données sont importées dans Power BI</li>
                </ol>
            </div>

            <h3>11.4 Création du dashboard "Vue d'ensemble des ventes"</h3>
            
            <div class="step-box">
                <h4><span class="step-number">4</span>Création de la page de rapport</h4>
                
                <ol>
                    <li>Vous êtes maintenant dans la vue "Rapport" de Power BI</li>
                    <li>En bas, vous voyez "Page 1"</li>
                    <li><div class="click-instruction">Double-cliquez sur "Page 1" et renommez-la en <strong>"Vue d'ensemble"</strong></div></li>
                    <li>Appuyez sur Entrée</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">5</span>Ajout d'une carte KPI : Chiffre d'affaires total</h4>
                
                <ol>
                    <li>Dans le panneau "Visualisations" (à droite), <div class="click-instruction">cliquez sur l'icône <strong>"Carte"</strong></div></li>
                    <li>Un visuel de carte vide apparaît sur le canevas</li>
                    <li>Dans le panneau "Champs" (à droite), développez <strong>MART_SALES_OVERVIEW</strong></li>
                    <li><div class="click-instruction">Cochez le champ <strong>TOTAL_REVENUE</strong></div></li>
                    <li>Le montant total du CA s'affiche dans la carte</li>
                    <li>Redimensionnez la carte en haut à gauche du canevas</li>
                    <li>Pour formater le montant :
                        <ul>
                            <li>Sélectionnez la carte</li>
                            <li>Dans le panneau "Visualisations", allez dans l'onglet "Format"</li>
                            <li>Développez <strong>"Étiquettes de données"</strong></li>
                            <li>Changez le format d'affichage en <strong>"€"</strong> avec 0 décimale</li>
                        </ul>
                    </li>
                    <li>Le KPI CA total est créé</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">6</span>Ajout d'une carte KPI : Nombre de commandes</h4>
                
                <ol>
                    <li><div class="click-instruction">Cliquez sur une zone vide du canevas pour désélectionner la carte précédente</div></li>
                    <li>Dans "Visualisations", <div class="click-instruction">cliquez sur <strong>"Carte"</strong></div></li>
                    <li>Dans "Champs", sous MART_SALES_OVERVIEW, <div class="click-instruction">cochez <strong>TOTAL_ORDERS</strong></div></li>
                    <li>Positionnez cette carte à côté de la première</li>
                    <li>Le KPI nombre de commandes est créé</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">7</span>Ajout d'un graphique en courbes : Évolution du CA</h4>
                
                <ol>
                    <li>Cliquez sur une zone vide du canevas</li>
                    <li>Dans "Visualisations", <div class="click-instruction">cliquez sur <strong>"Graphique en courbes"</strong></div></li>
                    <li>Dans "Champs", sous MART_SALES_OVERVIEW :
                        <ul>
                            <li>Glissez-déposez <strong>SALE_DATE</strong> dans <strong>"Axe X"</strong></li>
                            <li>Glissez-déposez <strong>TOTAL_REVENUE</strong> dans <strong>"Axe Y"</strong></li>
                        </ul>
                    </li>
                    <li>Le graphique d'évolution du CA s'affiche</li>
                    <li>Redimensionnez le graphique pour qu'il prenne toute la largeur sous les KPI</li>
                    <li>Le graphique d'évolution est créé</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">8</span>Ajout d'un graphique en barres : CA par pays</h4>
                
                <ol>
                    <li>Cliquez sur une zone vide du canevas</li>
                    <li>Dans "Visualisations", <div class="click-instruction">cliquez sur <strong>"Graphique à barres empilées"</strong></div></li>
                    <li>Dans "Champs", sous MART_SALES_OVERVIEW :
                        <ul>
                            <li>Glissez-déposez <strong>COUNTRY_CODE</strong> dans <strong>"Axe Y"</strong></li>
                            <li>Glissez-déposez <strong>TOTAL_REVENUE</strong> dans <strong>"Axe X"</strong></li>
                        </ul>
                    </li>
                    <li>Le graphique CA par pays s'affiche</li>
                    <li>Positionnez le graphique en bas à gauche</li>
                    <li>Le graphique par pays est créé</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">9</span>Ajout d'un graphique en secteurs : CA par catégorie</h4>
                
                <ol>
                    <li>Cliquez sur une zone vide du canevas</li>
                    <li>Dans "Visualisations", <div class="click-instruction">cliquez sur <strong>"Graphique en secteurs"</strong></div></li>
                    <li>Dans "Champs", sous MART_SALES_OVERVIEW :
                        <ul>
                            <li>Glissez-déposez <strong>PRODUCT_CATEGORY</strong> dans <strong>"Légende"</strong></li>
                            <li>Glissez-déposez <strong>TOTAL_REVENUE</strong> dans <strong>"Valeurs"</strong></li>
                        </ul>
                    </li>
                    <li>Le graphique CA par catégorie s'affiche</li>
                    <li>Positionnez le graphique en bas à droite</li>
                    <li>Le graphique par catégorie est créé</li>
                </ol>
            </div>

            <h3>11.5 Création de mesures DAX</h3>
            
            <div class="step-box">
                <h4><span class="step-number">10</span>Création d'une mesure : Panier moyen</h4>
                
                <ol>
                    <li>Dans le panneau "Champs", <div class="click-instruction">faites un clic droit sur <strong>MART_SALES_OVERVIEW</strong></div></li>
                    <li><div class="click-instruction">Sélectionnez <strong>"Nouvelle mesure"</strong></div></li>
                    <li>Dans la barre de formule en haut, tapez :
                        <pre><code>Panier Moyen = DIVIDE(SUM(MART_SALES_OVERVIEW[TOTAL_REVENUE]), SUM(MART_SALES_OVERVIEW[TOTAL_ORDERS]))</code></pre>
                    </li>
                    <li>Appuyez sur Entrée</li>
                    <li>La mesure "Panier Moyen" apparaît dans la liste des champs</li>
                    <li>Créez une nouvelle carte KPI avec cette mesure</li>
                    <li>La mesure Panier Moyen est créée</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">11</span>Création d'une mesure : CA du mois précédent</h4>
                
                <ol>
                    <li>Créez une nouvelle mesure sur MART_SALES_OVERVIEW</li>
                    <li>Tapez :
                        <pre><code>CA Mois Précédent = 
CALCULATE(
    SUM(MART_SALES_OVERVIEW[TOTAL_REVENUE]),
    DATEADD(MART_SALES_OVERVIEW[SALE_DATE], -1, MONTH)
)</code></pre>
                    </li>
                    <li>Appuyez sur Entrée</li>
                    <li>La mesure est créée</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">12</span>Création d'une mesure : Croissance mensuelle</h4>
                
                <ol>
                    <li>Créez une nouvelle mesure sur MART_SALES_OVERVIEW</li>
                    <li>Tapez :
                        <pre><code>Croissance Mensuelle = 
DIVIDE(
    SUM(MART_SALES_OVERVIEW[TOTAL_REVENUE]) - [CA Mois Précédent],
    [CA Mois Précédent],
    0
)</code></pre>
                    </li>
                    <li>Appuyez sur Entrée</li>
                    <li>Formatez la mesure en pourcentage (clic droit > Format > Pourcentage)</li>
                    <li>La mesure Croissance Mensuelle est créée</li>
                </ol>
            </div>

            <h3>11.6 Création d'une deuxième page : Performance Produits</h3>
            
            <div class="step-box">
                <h4><span class="step-number">13</span>Création de la page "Performance Produits"</h4>
                
                <ol>
                    <li>En bas de l'écran, <div class="click-instruction">cliquez sur le bouton <strong>"+"</strong> pour ajouter une nouvelle page</div></li>
                    <li>Renommez la page en <strong>"Performance Produits"</strong></li>
                    <li>Créez un tableau avec MART_PRODUCT_PERFORMANCE :
                        <ul>
                            <li>Dans "Visualisations", <div class="click-instruction">cliquez sur <strong>"Tableau"</strong></div></li>
                            <li>Ajoutez les colonnes :
                                <ul>
                                    <li>PRODUCT_NAME</li>
                                    <li>PRODUCT_CATEGORY</li>
                                    <li>TOTAL_REVENUE</li>
                                    <li>TOTAL_ORDERS</li>
                                    <li>ABC_CLASS</li>
                                    <li>PERFORMANCE_TIER</li>
                                </ul>
                            </li>
                        </ul>
                    </li>
                    <li>Redimensionnez le tableau pour qu'il prenne toute la page</li>
                    <li>Triez le tableau par TOTAL_REVENUE décroissant (cliquez sur l'en-tête de colonne)</li>
                    <li>Le tableau des performances produits est créé</li>
                </ol>
            </div>

            <h3>11.7 Création d'une troisième page : Analyse Clients</h3>
            
            <div class="step-box">
                <h4><span class="step-number">14</span>Création de la page "Analyse Clients"</h4>
                
                <ol>
                    <li>Ajoutez une nouvelle page et renommez-la en <strong>"Analyse Clients"</strong></li>
                    <li>Créez un graphique en anneau avec MART_CUSTOMER_LTV :
                        <ul>
                            <li>Dans "Visualisations", <div class="click-instruction">cliquez sur <strong>"Graphique en anneau"</strong></div></li>
                            <li>Légende : RFM_SEGMENT</li>
                            <li>Valeurs : Count of CUSTOMER_KEY</li>
                        </ul>
                    </li>
                    <li>Positionnez le graphique en haut à gauche</li>
                    <li>Créez un graphique en barres:
                        <ul>
                            <li>Dans "Visualisations", <div class="click-instruction">cliquez sur <strong>"Graphique à barres empilées"</strong></div></li>
                            <li>Axe Y : RFM_SEGMENT</li>
                            <li>Axe X : LIFETIME_VALUE (somme)</li>
                        </ul>
                    </li>
                    <li>Positionnez le graphique en haut à droite</li>
                    <li>Créez un tableau avec les top clients :
                        <ul>
                            <li>Dans "Visualisations", <div class="click-instruction">cliquez sur <strong>"Tableau"</strong></div></li>
                            <li>Ajoutez les colonnes :
                                <ul>
                                    <li>FULL_NAME</li>
                                    <li>EMAIL</li>
                                    <li>COUNTRY_CODE</li>
                                    <li>LIFETIME_VALUE</li>
                                    <li>TOTAL_ORDERS</li>
                                    <li>RFM_SEGMENT</li>
                                    <li>CHURN_RISK</li>
                                </ul>
                            </li>
                        </ul>
                    </li>
                    <li>Positionnez le tableau en bas</li>
                    <li>Triez par LIFETIME_VALUE décroissant</li>
                    <li>Le tableau d'analyse clients est créé</li>
                </ol>
            </div>

            <h3>11.8 Ajout de filtres interactifs</h3>
            
            <div class="step-box">
                <h4><span class="step-number">15</span>Ajout d'un filtre de date</h4>
                
                <ol>
                    <li>Retournez sur la page "Vue d'ensemble"</li>
                    <li>Dans le panneau "Visualisations", <div class="click-instruction">cliquez sur <strong>"Segment"</strong></div></li>
                    <li>Dans "Champs", sous MART_SALES_OVERVIEW, <div class="click-instruction">cochez <strong>SALE_DATE</strong></div></li>
                    <li>Un segment de date apparaît</li>
                    <li>Positionnez-le en haut de la page</li>
                    <li>Pour le rendre interactif :
                        <ul>
                            <li>Sélectionnez le segment</li>
                            <li>Dans "Visualisations", allez dans l'onglet "Format"</li>
                            <li>Développez <strong>"Style du segment"</strong></li>
                            <li>Sélectionnez <strong>"Entre"</strong> (permet de choisir une plage de dates)</li>
                        </ul>
                    </li>
                    <li>Le filtre de date est créé</li>
                    <li>Testez-le en sélectionnant une plage de dates : tous les visuels se mettent à jour automatiquement</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">16</span>Ajout d'un filtre de pays</h4>
                
                <ol>
                    <li>Cliquez sur une zone vide</li>
                    <li>Dans "Visualisations", <div class="click-instruction">cliquez sur <strong>"Segment"</strong></div></li>
                    <li>Dans "Champs", <div class="click-instruction">cochez <strong>COUNTRY_CODE</strong></div></li>
                    <li>Positionnez le segment à côté du filtre de date</li>
                    <li>Pour afficher les cases à cocher :
                        <ul>
                            <li>Sélectionnez le segment</li>
                            <li>Dans "Format", sous "Style du segment"</li>
                            <li>Sélectionnez <strong>"Liste déroulante"</strong></li>
                            <li>Activez <strong>"Sélection multiple"</strong></li>
                        </ul>
                    </li>
                    <li>Le filtre de pays est créé</li>
                </ol>
            </div>

            <h3>11.9 Ajout de visuels conditionnels</h3>
            
            <div class="step-box">
                <h4><span class="step-number">17</span>Mise en forme conditionnelle sur le tableau</h4>
                
                <ol>
                    <li>Allez sur la page "Analyse Clients"</li>
                    <li>Sélectionnez le tableau des clients</li>
                    <li>Dans le panneau "Visualisations", allez dans l'onglet "Format"</li>
                    <li>Développez <strong>"Valeurs de cellule"</strong></li>
                    <li>Trouvez la colonne <strong>CHURN_RISK</strong></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Couleur d'arrière-plan"</strong></div></li>
                    <li>Activez <strong>"Mise en forme conditionnelle"</strong></li>
                    <li>Configurez :
                        <ul>
                            <li>Format : <strong>Règles</strong></li>
                            <li>Règle 1 : Si la valeur est "High", couleur rouge clair</li>
                            <li>Règle 2 : Si la valeur est "Medium", couleur orange clair</li>
                            <li>Règle 3 : Si la valeur est "Low", couleur vert clair</li>
                        </ul>
                    </li>
                    <li><div class="click-instruction">Cliquez sur <strong>"OK"</strong></div></li>
                    <li>Les cellules de risque de churn sont maintenant colorées selon le niveau de risque</li>
                </ol>
            </div>

            <h3>11.10 Publication et partage</h3>
            
            <div class="step-box">
                <h4><span class="step-number">18</span>Enregistrement du rapport</h4>
                
                <ol>
                    <li>Dans le menu <strong>"Fichier"</strong>, <div class="click-instruction">cliquez sur <strong>"Enregistrer"</strong></div></li>
                    <li>Choisissez un emplacement (par exemple : <code>C:\Users\VotreNom\ShopStreamTP\powerbi\</code>)</li>
                    <li>Nom du fichier : <code>ShopStream_Dashboard. pbix</code></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Enregistrer"</strong></div></li>
                    <li>Le rapport est sauvegardé</li>
                </ol>
            </div>
            
            <div class="step-box">
                <h4><span class="step-number">19</span>Publication sur Power BI Service (optionnel)</h4>
                
                <div class="info-box">
                    <strong>Prérequis</strong> : Un compte Microsoft 365 ou un compte Power BI Pro/Premium
                </div>
                
                <ol>
                    <li>Dans Power BI Desktop, <div class="click-instruction">cliquez sur <strong>"Publier"</strong> dans le ruban "Accueil"</div></li>
                    <li>Connectez-vous avec votre compte Microsoft si demandé</li>
                    <li>Sélectionnez un espace de travail (par exemple : "Mon espace de travail")</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Sélectionner"</strong></div></li>
                    <li>Power BI publie le rapport (durée : 1-2 minutes)</li>
                    <li>Vous voyez : "Réussite - ShopStream_Dashboard.pbix a été publié"</li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Ouvrir dans Power BI"</strong></div></li>
                    <li>Votre navigateur s'ouvre sur Power BI Service</li>
                    <li>Vous pouvez maintenant partager le rapport avec d'autres utilisateurs</li>
                </ol>
            </div>

            <h3>11.11 Configuration du rafraîchissement automatique</h3>
            
            <div class="step-box">
                <h4><span class="step-number">20</span>Planification du rafraîchissement (Power BI Service)</h4>
                
                <ol>
                    <li>Dans Power BI Service, allez dans votre espace de travail</li>
                    <li>Trouvez le dataset <strong>"ShopStream_Dashboard"</strong></li>
                    <li><div class="click-instruction">Cliquez sur les trois points à droite du dataset</div></li>
                    <li><div class="click-instruction">Sélectionnez <strong>"Paramètres"</strong></div></li>
                    <li>Développez <strong>"Actualisation planifiée"</strong></li>
                    <li>Activez <strong>"Conserver vos données à jour"</strong></li>
                    <li>Configurez la fréquence :
                        <ul>
                            <li>Fréquence d'actualisation : <strong>Quotidienne</strong></li>
                            <li>Heure : <strong>08:00</strong> (après l'exécution du DAG Airflow à 2h)</li>
                        </ul>
                    </li>
                    <li>Activez <strong>"Envoyer une notification d'échec d'actualisation"</strong></li>
                    <li><div class="click-instruction">Cliquez sur <strong>"Appliquer"</strong></div></li>
                    <li>Le rafraîchissement automatique est configuré</li>
                    <li>Les données seront mises à jour automatiquement chaque matin</li>
                </ol>
            </div>

            <div class="success-box">
                <h4>Récapitulatif Power BI</h4>
                <p>Vous avez réussi à :</p>
                <ul>
                    <li>Installer Power BI Desktop</li>
                    <li>Connecter Power BI à Snowflake</li>
                    <li>Importer les tables data marts</li>
                    <li>Créer 3 pages de dashboards interactifs :
                        <ul>
                            <li>Vue d'ensemble des ventes (KPI, évolutions, répartitions)</li>
                            <li>Performance produits (tableau avec classification ABC)</li>
                            <li>Analyse clients (segmentation RFM, CLV, risque de churn)</li>
                        </ul>
                    </li>
                    <li>Créer des mesures DAX calculées</li>
                    <li>Ajouter des filtres interactifs</li>
                    <li>Configurer la mise en forme conditionnelle</li>
                    <li>Publier et planifier le rafraîchissement automatique</li>
                </ul>
                <p><strong>Le pipeline complet est maintenant opérationnel</strong></p>
            </div>

            <!-- Section 12 -->
            <h2 id="section12">12. Travail à rendre et livrables</h2>

            <div class="pipeline-context">
                <h3>Récapitulatif complet du pipeline</h3>
                <p>Vous avez construit un pipeline de données moderne de bout en bout :</p>
                <div class="pipeline-steps">
                    <div class="pipeline-step">PostgreSQL (OLTP)</div>
                    <div class="pipeline-step">S3 (Data Lake)</div>
                    <div class="pipeline-step">Snowflake (DWH)</div>
                    <div class="pipeline-step">dbt (Transform)</div>
                    <div class="pipeline-step">Airflow (Orchestration)</div>
                    <div class="pipeline-step">Power BI (BI)</div>
                </div>
                <p style="margin-top: 15px;">Chaque étape est automatisée et orchestrée.   Les données circulent de la source OLTP jusqu'aux dashboards décisionnels en passant par toutes les transformations nécessaires. </p>
            </div>

            <h3>12.1 Objectifs pédagogiques atteints</h3>
            <p>À la fin de ce TP, vous devez être capable de :</p>
            <ul>
                <li>Expliquer l'architecture d'un Modern Data Stack et le rôle de chaque composant</li>
                <li>Concevoir un schéma OLTP normalisé (PostgreSQL)</li>
                <li>Organiser un Data Lake dans S3 (partitionnement, formats)</li>
                <li>Charger des données dans Snowflake avec COPY INTO</li>
                <li>Écrire des modèles dbt (staging, dimensions, faits, marts)</li>
                <li>Orchestrer un pipeline avec Apache Airflow (DAG)</li>
                <li>Créer des dashboards BI connectés à Snowflake</li>
                <li>Argumenter les choix technologiques (OLTP vs OLAP, cloud vs on-premise)</li>
            </ul>

            <h3>12.2 Questions de réflexion</h3>
            
            <div class="step-box">
                <h4>Question 1 : Architecture et justification des choix</h4>
                
                <p><strong>Consigne :</strong></p>
                <p>Dessinez l'architecture complète du pipeline ShopStream (vous pouvez utiliser draw.io, Lucidchart, ou un outil similaire).  Pour chaque composant, expliquez :</p>
                <ul>
                    <li>Son rôle dans le pipeline</li>
                    <li>Pourquoi ce choix plutôt qu'une alternative (exemple : pourquoi Snowflake plutôt que PostgreSQL pour l'analytique ?)</li>
                    <li>Les flux de données entre composants (format, fréquence, volume estimé)</li>
                </ul>
                
                <p><strong>Éléments à inclure dans votre réponse :</strong></p>
                <ul>
                    <li>Diagramme d'architecture avec toutes les couches</li>
                    <li>Tableau comparatif justifiant chaque choix technologique</li>
                    <li>Estimation des coûts mensuels du pipeline en production (calculez le coût de chaque service)</li>
                </ul>
            </div>
            
            <div class="step-box">
                <h4>Question 2 : Comparaison avec une architecture on-premise</h4>
                
                <p><strong>Consigne :</strong></p>
                <p>Comparez l'architecture moderne (cloud) construite dans ce TP avec une architecture classique on-premise utilisant des outils comme Pentaho, Talend, ou Oracle Data Integrator.</p>
                
                <p><strong>Remplissez ce tableau comparatif :</strong></p>
                <table>
                    <tr>
                        <th>Critère</th>
                        <th>Architecture On-Premise (Pentaho)</th>
                        <th>Architecture Cloud (ce TP)</th>
                        <th>Avantage pour</th>
                    </tr>
                    <tr>
                        <td>Coût initial</td>
                        <td></td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Coût opérationnel (3 ans)</td>
                        <td></td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Scalabilité</td>
                        <td></td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Maintenance</td>
                        <td></td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Performance sur gros volumes</td>
                        <td></td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Compétences requises</td>
                        <td></td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Time-to-market</td>
                        <td></td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Sécurité des données</td>
                        <td></td>
                        <td></td>
                        <td></td>
                    </tr>
                </table>
                
                <p><strong>Conclusion :</strong> Dans quels contextes recommanderiez-vous chaque architecture ?</p>
            </div>
            
            <div class="step-box">
                <h4>Question 3 : Proposition de nouveaux KPIs</h4>
                
                <p><strong>Consigne :</strong></p>
                <p>Proposez 3 nouveaux KPIs métier pertinents pour ShopStream qui ne sont pas encore implémentés dans le pipeline. </p>
                
                <p><strong>Pour chaque KPI, fournissez :</strong></p>
                <ol>
                    <li><strong>Nom et définition</strong> : Quel est le KPI et que mesure-t-il ?</li>
                    <li><strong>Formule de calcul</strong> : Comment le calculer précisément ?</li>
                    <li><strong>Intérêt métier</strong> : Pourquoi ce KPI est-il important ?  Quelle décision aide-t-il à prendre ? </li>
                    <li><strong>Sources de données</strong> : Quelles tables du pipeline faut-il utiliser ?</li>
                    <li><strong>Requête SQL ou modèle dbt</strong> : Écrivez le code SQL pour calculer ce KPI</li>
                    <li><strong>Visualisation recommandée</strong> : Quel type de graphique Power BI utiliser ?</li>
                </ol>
                
                <p><strong>Exemples de KPIs possibles (n'utilisez pas ceux-ci, proposez les vôtres) :</strong></p>
                <ul>
                    <li>Taux de réachat à 30 jours</li>
                    <li>Coût d'acquisition client (CAC)</li>
                    <li>Ratio LTV/CAC</li>
                    <li>Taux d'abandon de panier</li>
                    <li>Délai moyen entre inscription et premier achat</li>
                </ul>
            </div>
            
            <div class="step-box">
                <h4>Question 4 : Plan de tests et qualité de données</h4>
                
                <p><strong>Consigne :</strong></p>
                <p>Rédigez un plan de tests complet (2-3 pages) pour garantir la qualité du pipeline ShopStream. </p>
                
                <p><strong>Votre plan doit inclure :</strong></p>
                
                <h4>A. Tests de données (Data Quality)</h4>
                <ul>
                    <li>Liste des règles métier à valider (exemples : email valide, montants positifs, dates cohérentes)</li>
                    <li>Pour chaque règle :
                        <ul>
                            <li>Table concernée</li>
                            <li>Colonne(s) testée(s)</li>
                            <li>Type de test (unicité, non-nullité, plage de valeurs, format)</li>
                            <li>Requête SQL de test</li>
                            <li>Seuil d'alerte (exemple : si plus de 1% d'erreurs, alerter)</li>
                        </ul>
                    </li>
                </ul>
                
                <h4>B. Tests de transformations</h4>
                <ul>
                    <li>Comment vérifier que les jointures dbt sont correctes ?</li>
                    <li>Comment valider que les agrégations sont justes ?</li>
                    <li>Comment tester la logique de calcul des KPIs ?</li>
                    <li>Proposez au moins 3 tests unitaires dbt avec le code YAML</li>
                </ul>
                
                <h4>C. Tests de bout en bout</h4>
                <ul>
                    <li>Décrivez un scénario de test E2E complet : de l'insertion d'une ligne dans PostgreSQL jusqu'à sa visualisation dans Power BI</li>
                    <li>Comment automatiser ce test ?</li>
                    <li>Combien de temps doit prendre ce test ?</li>
                </ul>
                
                <h4>D.  Tests de régression</h4>
                <ul>
                    <li>Comment s'assurer qu'une modification du code dbt ne casse pas les dashboards existants ?</li>
                    <li>Proposez une stratégie de versioning et de déploiement (dev, staging, prod)</li>
                </ul>
            </div>
            
            <div class="step-box">
                <h4>Question 5 : Sécurité et conformité RGPD</h4>
                
                <p><strong>Consigne :</strong></p>
                <p>ShopStream opère en Europe et doit respecter le RGPD (Règlement Général sur la Protection des Données).</p>
                
                <p><strong>Répondez aux questions suivantes :</strong></p>
                
                <h4>A.  Identification des données personnelles</h4>
                <ul>
                    <li>Quelles données du pipeline ShopStream sont considérées comme "données personnelles" au sens du RGPD ? </li>
                    <li>Classez-les par sensibilité (normale, sensible, très sensible)</li>
                </ul>
                
                <h4>B. Droit à l'oubli</h4>
                <ul>
                    <li>Un utilisateur demande la suppression complète de ses données.   Décrivez la procédure technique étape par étape pour honorer cette demande dans le pipeline</li>
                    <li>Quelles tables faut-il modifier dans PostgreSQL ? </li>
                    <li>Comment propager cette suppression dans Snowflake ?</li>
                    <li>Que faire des données historiques dans S3 ?</li>
                    <li>Écrivez le script SQL pour anonymiser un utilisateur spécifique</li>
                </ul>
                
                <h4>C. Anonymisation et pseudonymisation</h4>
                <ul>
                    <li>Proposez une stratégie pour anonymiser les emails dans le data warehouse tout en gardant la possibilité de faire des analyses de cohortes</li>
                    <li>Écrivez une fonction dbt pour hasher les emails de manière déterministe</li>
                    <li>Comment implémenter la pseudonymisation dans le pipeline ? </li>
                </ul>
                
                <h4>D.  Contrôle d'accès (Row-Level Security)</h4>
                <ul>
                    <li>Comment implémenter le RLS dans Snowflake pour que :
                        <ul>
                            <li>L'équipe marketing ne voit que les données européennes</li>
                            <li>Les analystes juniors ne voient pas les données financières sensibles</li>
                            <li>Chaque marchand ne voit que ses propres données</li>
                        </ul>
                    </li>
                    <li>Écrivez les commandes SQL Snowflake pour implémenter ces règles</li>
                </ul>
                
                <h4>E.  Audit et traçabilité</h4>
                <ul>
                    <li>Comment logger qui accède à quelles données et quand ?</li>
                    <li>Où stocker ces logs d'audit ?</li>
                    <li>Combien de temps les conserver ?</li>
                    <li>Comment automatiser la génération de rapports d'audit pour la CNIL ?</li>
                </ul>
            </div>

            <h3>12.3 Livrables attendus</h3>
            
            <div class="success-box">
                <h4>Livrable 1 : Document de synthèse (PDF)</h4>
                <p><strong>Format :</strong> PDF de 15-20 pages</p>
                <p><strong>Contenu :</strong></p>
                <ul>
                    <li>Page de garde (titre, auteurs, date)</li>
                    <li>Sommaire</li>
                    <li>Introduction : contexte ShopStream, objectifs du pipeline</li>
                    <li>Architecture détaillée avec diagrammes annotés</li>
                    <li>Description technique de chaque composant :
                        <ul>
                            <li>PostgreSQL : schéma, génération de données</li>
                            <li>S3 : organisation, scripts d'export</li>
                            <li>Snowflake : configuration, chargement</li>
                            <li>dbt : modèles, tests, documentation</li>
                            <li>Airflow : DAG, orchestration</li>
                            <li>Power BI : dashboards, mesures DAX</li>
                        </ul>
                    </li>
                    <li>Extraits de code clés avec explications</li>
                    <li>Captures d'écran des dashboards Power BI</li>
                    <li>Réponses détaillées aux 5 questions de réflexion</li>
                    <li>Conclusion : bilan, difficultés rencontrées, améliorations possibles</li>
                    <li>Annexes : code complet, logs d'exécution</li>
                </ul>
            </div>
            
            <div class="success-box">
                <h4>Livrable 2 : Dépôt Git</h4>
                <p><strong>Format :</strong> Repository GitHub/GitLab public ou privé</p>
                <p><strong>Structure recommandée :</strong></p>
                <pre><code>shopstream-data-pipeline/
├── README.md
├── docs/
│   ├── architecture.md
│   ├── setup_guide.  md
│   └── user_manual.md
├── sql/
│   ├── postgresql/
│   │   └── create_tables.sql
│   └── snowflake/
│       ├── setup. sql
│       └── load_staging.sql
├── scripts/
│   ├── generate_data.py
│   ├── export_to_s3.py
│   └── requirements.txt
├── dbt/
│   └── shopstream_dbt/
│       ├── models/
│       │   ├── staging/
│       │   ├── core/
│       │   └── marts/
│       ├── tests/
│       └── dbt_project.yml
├── airflow/
│   └── dags/
│       └── shopstream_daily_pipeline.py
├── powerbi/
│   ├── ShopStream_Dashboard.pbix
│   └── screenshots/
│       ├── dashboard_sales. png
│       ├── dashboard_products.png
│       └── dashboard_customers.png
└── . gitignore
</code></pre>
                
                <p><strong>Le README.md doit contenir :</strong></p>
                <ul>
                    <li>Description du projet</li>
                    <li>Architecture (avec diagramme)</li>
                    <li>Prérequis techniques</li>
                    <li>Instructions d'installation étape par étape</li>
                    <li>Instructions d'exécution</li>
                    <li>Captures d'écran des résultats</li>
                    <li>Technologies utilisées</li>
                    <li>Auteurs et remerciements</li>
                </ul>
            </div>
            
            <div class="success-box">
                <h4>Livrable 3 : Présentation (Slides + Vidéo)</h4>
                <p><strong>Format :</strong> PowerPoint/Google Slides + Vidéo de démonstration</p>
                
                <p><strong>Slides (15-20 slides) :</strong></p>
                <ul>
                    <li>Slide 1 : Titre, auteurs, date</li>
                    <li>Slide 2 : Contexte métier ShopStream</li>
                    <li>Slide 3 : Problématique et objectifs</li>
                    <li>Slides 4-6 : Architecture du pipeline (diagrammes)</li>
                    <li>Slides 7-12 : Zoom sur chaque composant (1 slide = 1 techno)</li>
                    <li>Slides 13-15 : Démonstration (captures d'écran des dashboards)</li>
                    <li>Slide 16 : Comparaison on-premise vs cloud</li>
                    <li>Slide 17 : Sécurité et RGPD</li>
                    <li>Slide 18 : Difficultés et solutions</li>
                    <li>Slide 19 : Améliorations futures</li>
                    <li>Slide 20 : Conclusion et questions</li>
                </ul>
                
                <p><strong>Vidéo de démonstration (5-10 minutes) :</strong></p>
                <ul>
                    <li>Enregistrez votre écran avec OBS Studio ou Loom</li>
                    <li>Montrez le pipeline en action :
                        <ul>
                            <li>Insertion de données dans PostgreSQL</li>
                            <li>Vérification dans S3</li>
                            <li>Vérification dans Snowflake</li>
                            <li>Exécution manuelle du DAG Airflow</li>
                            <li>Visite des dashboards Power BI</li>
                            <li>Interaction avec les filtres</li>
                        </ul>
                    </li>
                    <li>Ajoutez une voix off expliquant ce que vous faites</li>
                    <li>Publiez la vidéo sur YouTube (non listée) ou Vimeo</li>
                </ul>
            </div>

            <h3>12.4 Critères d'évaluation</h3>
            
            <table>
                <tr>
                    <th>Critère</th>
                    <th>Points</th>
                    <th>Description</th>
                </tr>
                <tr>
                    <td><strong>Compréhension de l'architecture</strong></td>
                    <td>20</td>
                    <td>Capacité à expliquer le rôle de chaque composant, les flux de données, les choix technologiques</td>
                </tr>
                <tr>
                    <td><strong>Qualité du code</strong></td>
                    <td>25</td>
                    <td>SQL propre et commenté, Python documenté, modèles dbt structurés, DAG Airflow fonctionnel</td>
                </tr>
                <tr>
                    <td><strong>Dashboards BI</strong></td>
                    <td>15</td>
                    <td>Pertinence des KPIs, lisibilité, esthétique, interactivité, utilité pour le métier</td>
                </tr>
                <tr>
                    <td><strong>Réflexion critique</strong></td>
                    <td>20</td>
                    <td>Qualité des réponses aux 5 questions (comparaison, KPIs, tests, RGPD), profondeur d'analyse</td>
                </tr>
                <tr>
                    <td><strong>Documentation</strong></td>
                    <td>10</td>
                    <td>Clarté du README, commentaires dans le code, diagrammes annotés, guide d'installation</td>
                </tr>
                <tr>
                    <td><strong>Présentation orale</strong></td>
                    <td>10</td>
                    <td>Clarté de l'exposé, gestion du temps, qualité de la démonstration vidéo, réponses aux questions</td>
                </tr>
                <tr>
                    <td><strong>TOTAL</strong></td>
                    <td><strong>100</strong></td>
                    <td></td>
                </tr>
            </table>

            <h3>12.5 Bonus (points supplémentaires)</h3>
            <ul>
                <li><strong>+10 points</strong> : Implémentation complète d'un modèle de Machine Learning intégré au pipeline (ex : prédiction du churn avec Snowflake ML ou Python)</li>
                <li><strong>+8 points</strong> : Mise en place d'un framework de Data Quality complet (Great Expectations ou Soda Core) avec rapports automatiques</li>
                <li><strong>+5 points</strong> : CI/CD pour dbt avec GitHub Actions (tests automatiques sur chaque PR)</li>
                <li><strong>+5 points</strong> : Documentation dbt avancée avec descriptions, tests custom, macros réutilisables</li>
                <li><strong>+5 points</strong> : Implémentation de Slowly Changing Dimensions (SCD Type 2) avec dbt snapshots</li>
                <li><strong>+3 points</strong> : Monitoring avancé du pipeline avec Grafana ou DataDog</li>
                <li><strong>+3 points</strong> : Implémentation du Row-Level Security dans Snowflake avec démonstration</li>
            </ul>

            <h3>12.6 Ressources complémentaires</h3>
            
            <h4>Documentation officielle</h4>
            <ul>
                <li><a href="https://www.postgresql.  org/docs/" target="_blank">PostgreSQL Documentation</a></li>
                <li><a href="https://docs.aws.amazon.com/s3/" target="_blank">Amazon S3 Documentation</a></li>
                <li><a href="https://docs.snowflake.com/" target="_blank">Snowflake Documentation</a></li>
                <li><a href="https://docs.getdbt.com/" target="_blank">dbt Documentation</a></li>
                <li><a href="https://airflow.  apache.org/docs/" target="_blank">Apache Airflow Documentation</a></li>
                <li><a href="https://learn.microsoft.com/en-us/power-bi/" target="_blank">Power BI Documentation</a></li>
            </ul>
            
            <h4>Livres recommandés</h4>
            <ul>
                <li><strong>"The Data Warehouse Toolkit"</strong> - Ralph Kimball (modélisation dimensionnelle)</li>
                <li><strong>"Designing Data-Intensive Applications"</strong> - Martin Kleppmann (architectures distribuées)</li>
                <li><strong>"Fundamentals of Data Engineering"</strong> - Joe Reis & Matt Housley (modern data stack)</li>
                <li><strong>"Analytics Engineering with SQL and dbt"</strong> - Rui Machado (dbt en profondeur)</li>
                <li><strong>"Data Pipelines Pocket Reference"</strong> - James Densmore (bonnes pratiques)</li>
            </ul>
            
            <h4>Tutoriels vidéo</h4>
            <ul>
                <li><a href="https://www.youtube.com/c/dbt-labs" target="_blank">Chaîne YouTube dbt Labs</a></li>
                <li><a href="https://www.youtube.com/c/Snowflake" target="_blank">Chaîne YouTube Snowflake</a></li>
                <li><a href="https://www.youtube.com/c/ApacheAirflow" target="_blank">Chaîne YouTube Apache Airflow</a></li>
                <li><a href="https://www.youtube.com/c/MicrosoftPowerBI" target="_blank">Chaîne YouTube Power BI</a></li>
            </ul>
            
            <h4>Communautés</h4>
            <ul>
                <li><a href="https://www.getdbt.com/community/" target="_blank">dbt Community Slack</a> (plus de 60 000 membres)</li>
                <li><a href="https://community.snowflake.com/" target="_blank">Snowflake Community</a></li>
                <li><a href="https://stackoverflow.com/questions/tagged/apache-airflow" target="_blank">Stack Overflow - Apache Airflow</a></li>
                <li><a href="https://www.reddit.  com/r/dataengineering/" target="_blank">Reddit r/dataengineering</a></li>
                <li><a href="https://community.powerbi.com/" target="_blank">Power BI Community</a></li>
            </ul>

          

            <div class="info-box">
                <strong>Conseils pour réussir</strong>
                <ul>
                    <li>Commencez tôt : N'attendez pas la dernière semaine</li>
                    <li>Testez au fur et à mesure : Validez chaque étape avant de passer à la suivante</li>
                    <li>Documentez en continu : Prenez des notes et captures d'écran pendant le développement</li>
                    <li>Travaillez en équipe : Répartissez les tâches si c'est un projet de groupe</li>
                    <li>Utilisez Git : Commits réguliers, branches pour chaque feature</li>
                    <li>Demandez de l'aide : Forums, Slack, enseignant - n'hésitez pas</li>
                    <li>Privilégiez la compréhension : Mieux vaut un pipeline simple mais bien expliqué qu'un pipeline complexe non maîtrisé</li>
                    <li>Relisez vos livrables : Traquez les fautes, vérifiez la cohérence</li>
                </ul>
            </div>

            <h3>12.8 FAQ et dépannage</h3>
            
            <div class="warning-box">
                <h4>Problèmes courants et solutions</h4>
                
                <p><strong>Q : Erreur de connexion PostgreSQL depuis Python</strong></p>
                <p>R : Vérifiez que PostgreSQL est démarré, que le mot de passe est correct, et que le port 5432 est bien ouvert.</p>
                
                <p><strong>Q : AWS CLI ne reconnaît pas mes credentials</strong></p>
                <p>R : Relancez <code>aws configure</code> et vérifiez que les clés sont correctement collées (pas d'espace en début/fin).</p>
                
                <p><strong>Q : Snowflake ne trouve pas mon stage S3</strong></p>
                <p>R : Vérifiez la Storage Integration (DESC INTEGRATION), la relation de confiance IAM, et que le bucket/région sont corrects.</p>
                
                <p><strong>Q : dbt ne trouve pas mes sources</strong></p>
                <p>R : Vérifiez le fichier <code>_staging__sources.yml</code>, la connexion dans <code>profiles.yml</code>, et lancez <code>dbt debug</code>.</p>
                
                <p><strong>Q : Le DAG Airflow n'apparaît pas dans l'interface</strong></p>
                <p>R : Vérifiez que le fichier est bien dans <code>dags/</code>, qu'il n'y a pas d'erreur de syntaxe Python, et rafraîchissez la page. </p>
                
                <p><strong>Q : Power BI ne se connecte pas à Snowflake</strong></p>
                <p>R : Vérifiez l'URL du serveur (sans https://), le nom du warehouse, et que l'utilisateur a les droits d'accès.</p>
                
                <p><strong>Q : Les données ne se rafraîchissent pas automatiquement</strong></p>
                <p>R : Vérifiez le schedule du DAG Airflow, que le scheduler est lancé, et que les connexions sont valides.</p>
            </div>

            <hr style="margin: 50px 0;">

            <h2 style="text-align: center; color: #667eea;">Conclusion</h2>
            
            <p style="text-align: center; font-size: 1.1em; margin: 30px 0;">
                Ce TP vous a permis de découvrir et de manipuler un <strong>Modern Data Stack</strong> complet, 
                représentatif des architectures utilisées par les entreprises innovantes aujourd'hui.
            </p>
            
            <p style="text-align: center; font-size: 1.  1em; margin: 30px 0;">
                Vous avez appris à orchestrer un pipeline de données de bout en bout : 
                de la source OLTP (PostgreSQL) jusqu'aux dashboards décisionnels (Power BI), 
                en passant par le Data Lake (S3), le Data Warehouse (Snowflake), 
                les transformations analytiques (dbt) et l'orchestration (Airflow).
            </p>
            
            <p style="text-align: center; font-size: 1. 1em; margin: 30px 0;">
                Ces compétences sont <strong>très recherchées sur le marché</strong> : 
                Analytics Engineer, Data Engineer, Solutions Architect, BI Developer.   
                Les entreprises migrent massivement vers le cloud et ont besoin de profils maîtrisant ces technologies.
            </p>
            
            <div class="success-box" style="margin: 40px 0; padding: 30px;">
                <h3 style="text-align: center; margin-bottom: 20px;">Pour aller plus loin</h3>
                <ul style="font-size: 1.05em;">
                    <li><strong>Certifications recommandées</strong> :
                        <ul>
                            <li>Snowflake SnowPro Core Certification</li>
                            <li>AWS Certified Data Analytics - Specialty</li>
                            <li>dbt Analytics Engineering Certification</li>
                            <li>Microsoft Certified: Power BI Data Analyst Associate</li>
                            <li>Astronomer Certification for Apache Airflow</li>
                        </ul>
                    </li>
                    <li><strong>Projets personnels</strong> :
                        <ul>
                            <li>Construire votre propre pipeline sur des données publiques (Kaggle, data.gouv.fr, Open Data)</li>
                            <li>Contribuer à des projets open-source (dbt packages, Airflow providers, Great Expectations)</li>
                            <li>Publier vos dashboards et analyses sur un portfolio en ligne (GitHub Pages, Notion)</li>
                            <li>Participer à des hackathons data</li>
                        </ul>
                    </li>
                    <li><strong>Communauté</strong> :
                        <ul>
                            <li>Participer à des meetups Data Engineering dans votre ville (Meetup. com)</li>
                            <li>Suivre des experts sur LinkedIn et Twitter</li>
                            <li>Lire des newsletters spécialisées (Data Engineering Weekly, dbt Roundup, Airflow Newsletter)</li>
                            <li>Rejoindre des Slacks communautaires (dbt, Airflow, Locally Optimistic)</li>
                        </ul>
                    </li>
                    <li><strong>Veille technologique</strong> :
                        <ul>
                            <li>Suivre les annonces des conférences (Snowflake Summit, AWS re:Invent, Coalesce by dbt)</li>
                            <li>Lire des blogs techniques (dbt Labs, Airbyte, Fivetran, Hightouch)</li>
                            <li>Expérimenter avec de nouveaux outils (Dagster, Prefect, Mage.ai)</li>
                        </ul>
                    </li>
                </ul>
            </div>
            
            <p style="text-align: center; font-size: 1.3em; margin: 50px 0; font-weight: bold; color: #667eea;">
                Bon courage pour la réalisation de ce TP<br>
                N'hésitez pas à poser des questions et à expérimenter
            </p>

        </div>
        
        <footer>
            <p>2025 - TP Cloud : Pipeline moderne PostgreSQL vers S3 vers Snowflake vers dbt vers Airflow vers BI</p>
            <p>Pr AMAMOU Ahmed</p>
        </footer>
    </div>

    <script>
        // Initialisation de Mermaid pour les diagrammes
        mermaid.initialize({ 
            startOnLoad: true,
            theme: 'default',
            securityLevel: 'loose',
            flowchart: {
                useMaxWidth: true,
                htmlLabels: true,
                curve: 'basis'
            }
        });
    </script>
</body>
</html>